{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "from gensim import corpora, models, matutils\n",
    "import matplotlib.pyplot as plt\n",
    "from medtop.helpers import *\n",
    "from medtop.nlp_helpers import *\n",
    "from medtop.preprocessing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import ward, cut_tree\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from textblob import TextBlob\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Methods intended to be called publicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_docs(path_to_file_list):\n",
    "    \"Imports and processes the list of documents contained in the given file\"\n",
    "    \n",
    "    # Extract list of files from text document\n",
    "    with open(path_to_file_list) as file:\n",
    "        file_list = file.read().strip().split('\\n')\n",
    "    \n",
    "    # Create a dataframe containing the doc_id, file name, and raw text of each document\n",
    "    doc_cols = [\"doc_id\", \"file\", \"text\", \"tokens\"]\n",
    "    doc_df = pd.DataFrame(columns=doc_cols)\n",
    "    \n",
    "    # Create a dataframe containing the doc_sent_id, doc_id, sent_id, raw text, tokens, and PoS tags for each sentence\n",
    "    sent_cols = [\"doc_sent_id\", \"doc_id\", \"sent_id\", \"text\", \"tokens\", \"pos_tags\"]\n",
    "    data = pd.DataFrame(columns=sent_cols)\n",
    "    \n",
    "    for doc_id, file in enumerate(file_list):\n",
    "        with open(file, \"r\") as file_content:\n",
    "            doc_text = file_content.read()\n",
    "            \n",
    "        sent_tokens, sent_pos, raw_sent = tokenize_and_stem(doc_text)\n",
    "        for sent_id, _ in enumerate(sent_tokens):\n",
    "            # Populate a row in data for each sentence in the document\n",
    "            doc_sent_id = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            sent_text = raw_sent[sent_id]\n",
    "            tokens = sent_tokens[sent_id]\n",
    "            pos_tags = sent_pos[sent_id]\n",
    "            sent_row = pd.Series([doc_sent_id, doc_id, sent_id, sent_text, tokens, pos_tags], index=sent_cols)\n",
    "            data = data.append(sent_row, ignore_index=True)\n",
    "        \n",
    "        # Populate a row in doc_df for each file loaded\n",
    "        doc_tokens = [token for sent in data[data.doc_id==doc_id].tokens for token in sent]\n",
    "        doc_row = pd.Series([doc_id, file, doc_text, doc_tokens], index=doc_cols)\n",
    "        doc_df = doc_df.append(doc_row, ignore_index=True)\n",
    "    \n",
    "    return data, doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sentences_to_disk(data, file_name = 'output/DocumentSentenceList.txt'):\n",
    "    \"Writes the raw sentences to a file organized by document and sentence number\"\n",
    "    data.to_csv(file_name, columns=[\"doc_sent_id\", \"text\"], sep='\\t', encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tfidf(path_to_seed_topics_file_list, input_doc_df = None):\n",
    "    \"Creates a TF-IDF matrix from the tokens in the seed topics documents and optionally, the input corpus\"\n",
    "    \n",
    "    # Import seed topics documents\n",
    "    _, seed_doc_df = import_docs(path_to_seed_topics_file_list)\n",
    "     \n",
    "    # Bag of Words (BoW) is a list of all tokens by document\n",
    "    bow_docs = list(seed_doc_df.tokens)\n",
    "    \n",
    "    if input_doc_df is not None:\n",
    "        bow_docs = bow_docs + list(input_doc_df.tokens)\n",
    "\n",
    "    # Create a TF-IDF matrix using document tokens and gensim\n",
    "    dictionary = corpora.Dictionary(bow_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in bow_docs]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    num_terms = len(corpus_tfidf.obj.idfs)\n",
    "    tfidf_dense = matutils.corpus2dense(corpus_tfidf, num_terms)\n",
    "    print(f\"Size of TF-IDF: {len(tfidf_dense[0])} Docs, {len(tfidf_dense)} Tokens\")\n",
    "    \n",
    "    return tfidf_dense, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TF-IDF: 14 Docs, 188 Tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_sent_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc.0.sent.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I am currently on my first AI in the Emergency...</td>\n",
       "      <td>[currently, emergency, department]</td>\n",
       "      <td>[[currently, RB], [Emergency, NNP], [Departmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc.1.sent.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>I am currently working in the cardiac surgery ...</td>\n",
       "      <td>[currently, working, cardiac, surgery, icu, gr...</td>\n",
       "      <td>[[currently, RB], [working, VBG], [cardiac, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc.2.sent.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>As many have said, reading these messages whil...</td>\n",
       "      <td>[said, reading, message, starting, rotation, c...</td>\n",
       "      <td>[[said, VBD], [reading, VBG], [messages, NNS],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc.3.sent.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Very interesting posts by you guys, I really e...</td>\n",
       "      <td>[interesting, post, guy, enjoyed, reading]</td>\n",
       "      <td>[[interesting, JJ], [posts, NNS], [guys, VBP],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc.4.sent.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Just have to say that all of you seem to be do...</td>\n",
       "      <td>[seem, wonderfully, rotation]</td>\n",
       "      <td>[[seem, VBP], [wonderfully, RB], [rotations, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc.5.sent.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm currently on the MRICU rotation, after com...</td>\n",
       "      <td>[currently, mricu, rotation, completing, inter...</td>\n",
       "      <td>[[currently, RB], [MRICU, NNP], [rotation, NN]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>doc.6.sent.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>The greatest (but most fun and rewarding) chal...</td>\n",
       "      <td>[greatest, fun, rewarding, challenge, faced, t...</td>\n",
       "      <td>[[greatest, JJS], [fun, JJ], [rewarding, VBG],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>doc.7.sent.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>Hope you're all having a great AI experience s...</td>\n",
       "      <td>[hope, great, experience, far]</td>\n",
       "      <td>[[Hope, NN], [great, JJ], [experience, NN], [f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doc.8.sent.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>Anyhow, because of the odd nature of my schedu...</td>\n",
       "      <td>[anyhow, odd, nature, schedule, completed, sin...</td>\n",
       "      <td>[[Anyhow, NNP], [odd, JJ], [nature, NN], [sche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doc.9.sent.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>I am currently completing my ICU AI requiremen...</td>\n",
       "      <td>[currently, completing, icu, requirement, picu...</td>\n",
       "      <td>[[currently, RB], [completing, VBG], [ICU, NNP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_sent_id doc_id sent_id  \\\n",
       "0  doc.0.sent.0      0       0   \n",
       "1  doc.1.sent.0      1       0   \n",
       "2  doc.2.sent.0      2       0   \n",
       "3  doc.3.sent.0      3       0   \n",
       "4  doc.4.sent.0      4       0   \n",
       "5  doc.5.sent.0      5       0   \n",
       "6  doc.6.sent.0      6       0   \n",
       "7  doc.7.sent.0      7       0   \n",
       "8  doc.8.sent.0      8       0   \n",
       "9  doc.9.sent.0      9       0   \n",
       "\n",
       "                                                text  \\\n",
       "0  I am currently on my first AI in the Emergency...   \n",
       "1  I am currently working in the cardiac surgery ...   \n",
       "2  As many have said, reading these messages whil...   \n",
       "3  Very interesting posts by you guys, I really e...   \n",
       "4  Just have to say that all of you seem to be do...   \n",
       "5  I'm currently on the MRICU rotation, after com...   \n",
       "6  The greatest (but most fun and rewarding) chal...   \n",
       "7  Hope you're all having a great AI experience s...   \n",
       "8  Anyhow, because of the odd nature of my schedu...   \n",
       "9  I am currently completing my ICU AI requiremen...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                 [currently, emergency, department]   \n",
       "1  [currently, working, cardiac, surgery, icu, gr...   \n",
       "2  [said, reading, message, starting, rotation, c...   \n",
       "3         [interesting, post, guy, enjoyed, reading]   \n",
       "4                      [seem, wonderfully, rotation]   \n",
       "5  [currently, mricu, rotation, completing, inter...   \n",
       "6  [greatest, fun, rewarding, challenge, faced, t...   \n",
       "7                     [hope, great, experience, far]   \n",
       "8  [anyhow, odd, nature, schedule, completed, sin...   \n",
       "9  [currently, completing, icu, requirement, picu...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [[currently, RB], [Emergency, NNP], [Departmen...  \n",
       "1  [[currently, RB], [working, VBG], [cardiac, NN...  \n",
       "2  [[said, VBD], [reading, VBG], [messages, NNS],...  \n",
       "3  [[interesting, JJ], [posts, NNS], [guys, VBP],...  \n",
       "4  [[seem, VBP], [wonderfully, RB], [rotations, N...  \n",
       "5  [[currently, RB], [MRICU, NNP], [rotation, NN]...  \n",
       "6  [[greatest, JJS], [fun, JJ], [rewarding, VBG],...  \n",
       "7  [[Hope, NN], [great, JJ], [experience, NN], [f...  \n",
       "8  [[Anyhow, NNP], [odd, JJ], [nature, NN], [sche...  \n",
       "9  [[currently, RB], [completing, VBG], [ICU, NNP...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# path_to_file_list = 'data/2019.03.11_DevCorpus/file_list.txt'\n",
    "# data, doc_df = import_docs(path_to_file_list)\n",
    "# path_to_seed_topics_file_list = 'data/2019.03.12_SEED_TOPICS/FILELIST.txt'\n",
    "# tfidf, dictionary = create_tfidf(path_to_seed_topics_file_list, doc_df)\n",
    "# feature_names = dictionary.token2id\n",
    "# tdm = tfidf\n",
    "# append_phrase_column(data, feature_names, tdm)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def append_phrase_column(data, feature_names, tdm, window_size = 6, include_input_in_tfidf = False):\n",
    "    \"\"\"\n",
    "    Extracts the most expressive phrase of length `window_size` from each sentence and appends them to the input dataframe in a new 'phrase' column. \n",
    "    Sentences without phrases of sufficient length are set to None.\n",
    "    \"\"\"\n",
    "    adj_adv_pos_list = [\"JJ\",\"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]\n",
    "    top_phrases = []\n",
    "    \n",
    "    if not include_input_in_tfidf:\n",
    "        token_averages = get_term_max(tdm)\n",
    "        \n",
    "    # Iterate sentences\n",
    "    for ix, sent in data.iterrows():\n",
    "        # Iterate phrases (sub-sentences of length window_size)\n",
    "        phrase_scores = []\n",
    "        for p in range(len(sent.tokens) - window_size + 1):\n",
    "            window = slice(p, p + window_size)\n",
    "            phrase = sent.tokens[window]\n",
    "            phrase_pos = sent.pos_tags[window]\n",
    "\n",
    "            weight = 1 + abs(TextBlob(\" \".join(phrase)).sentiment.polarity)\n",
    "            score = 0\n",
    "\n",
    "            for i, token in enumerate(phrase):\n",
    "                # Skip tokens not in feature_names\n",
    "                if token not in list(feature_names.keys()):\n",
    "                    continue\n",
    "\n",
    "                pos = phrase_pos[i][1]\n",
    "                token_ix = feature_names[token]\n",
    "\n",
    "                # Token score comes from TF-IDf matrix if include_input_in_tfidf is set, otherwise, use tokens averages\n",
    "                token_score = tdm[token_ix, sent.doc_id] if include_input_in_tfidf else token_averages[token_ix];\n",
    "\n",
    "                # Scale token_score by 3x if the token is an adjective or adverb\n",
    "                score += (token_score * 3) if pos in adj_adv_pos_list else token_score\n",
    "\n",
    "            phrase_scores.append((score*weight, phrase))\n",
    "\n",
    "        # Select the phrase with the highest score, use None if there are no phrases\n",
    "        phrase_scores.sort(key=lambda sent: sent[0], reverse=True)\n",
    "        top_phrase = phrase_scores[0][1] if len(phrase_scores) > 0 else None\n",
    "        top_phrases.append(top_phrase) \n",
    "    \n",
    "    # Append a phrase column to the 'data' dataframe\n",
    "    data['phrase'] = top_phrases\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_word_vectors_tfidf(doc_top_phrases, dictionary, tfidf_dense):\n",
    "    \"Uses the TF-IDF matrix to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_dense)\n",
    "\n",
    "def get_doc_word_vectors_svd(doc_top_phrases, dictionary, tfidf_dense, dimensions = 2):\n",
    "    \"Decomposes the TF-IDF matrix via SVD and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    svd =  TruncatedSVD(n_components = dimensions, random_state = 42)\n",
    "    tfidf_transf = svd.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_transf)\n",
    "\n",
    "def get_doc_word_vectors_umap(doc_top_phrases, dictionary, tfidf_dense, umap_neighbors = 15, dimensions = 2):\n",
    "    \"Transforms the TF-IDF matrix via UMAP and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components = dimensions)\n",
    "    embed = reducer.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, embed)\n",
    "\n",
    "def get_doc_word_vectors_pretrained(doc_top_phrases, path_to_w2v_bin_file):\n",
    "    \"Uses pretrained Word2Vec embeddings to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    assert os.path.exists(path_to_w2v_bin_file), f'The file {path_to_w2v_bin_file} does not exist'\n",
    "    model = w2v_pretrained(path_to_w2v_bin_file)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)\n",
    "\n",
    "def get_doc_word_vectors_local(doc_top_phrases, raw_docs, my_docs):\n",
    "    \"Creates Word2Vec embeddings from the input corpus and uses them to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    model = w2v_from_corpus(raw_docs, my_docs)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_sentences(doc_phrase_vecs, doc_top_phrases):\n",
    "    \"Filter the sentences, removing any that contain zero 'top phrases'\"\n",
    "    just_phrase_vecs = []\n",
    "    just_phrase_ids = []\n",
    "    just_phrase_text = []\n",
    "\n",
    "    # Getting all phrases which have \n",
    "    # Iterate documents\n",
    "    for doc_ix, phrase_vecs in enumerate(doc_phrase_vecs):\n",
    "        # Iterate sentences (each sentence has a phrase vector)\n",
    "        for phrase_ix, phrase_vec in enumerate(phrase_vecs):\n",
    "            phrase_id = f\"doc.{doc_ix}.sent.{phrase_ix}\"\n",
    "            this_text = doc_top_phrases[doc_ix][phrase_ix]\n",
    "\n",
    "            # Check phrase_vec is not zero or an array of zeros, these would imply the sentence has no \"top phrases\"\n",
    "            if isinstance(phrase_vec, np.ndarray) and any(np.zeros(len(phrase_vec)) != phrase_vec):\n",
    "                just_phrase_vecs.append(list(phrase_vec))\n",
    "                just_phrase_ids.append(phrase_id)\n",
    "                just_phrase_text.append(this_text[1])\n",
    "    return just_phrase_vecs, just_phrase_ids, just_phrase_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_optimal_k(just_phrase_vecs, save_chart = False, chart_file = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors for a range of k values and plots the silhoute coefficients \n",
    "    (this is a function of mean intra-cluster distance and mean nearest-cluster distance). \n",
    "    Returns the optimal k-value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_k = min(len(just_phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [(silhouette_score(just_phrase_vecs, KMeans(i).fit(just_phrase_vecs).predict(just_phrase_vecs))) for i in k_range]\n",
    "    fig = plt.plot(k_range, score)\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(k, just_phrase_vecs):\n",
    "    \"Use K-means algorithm to cluster phrase vectors\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(just_phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(just_phrase_vecs)\n",
    "    #TODO: this is never used before being overwritten later. Remove?\n",
    "    dist = pairwise_distances(just_phrase_vecs, metric='euclidean') \n",
    "    return cluster_assignments, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_linkage_matrix(just_phrase_vecs, dist_metric):\n",
    "    \"Creates a linkage matrix by calculating distance between phrase vectors\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(just_phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(just_phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(just_phrase_vecs, linkage_matrix, save_chart = False, chart_file = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_h = min(len(just_phrase_vecs), 100)\n",
    "    # QUESTION: What is the limiting factor for the min/max in this range?\n",
    "#     h_range = range(30, max_h) \n",
    "    h_range = range(1,10) \n",
    "    h_score = [(silhouette_score(just_phrase_vecs, [x[0] for x in cut_tree(linkage_matrix, height=i)] )) for i in h_range]\n",
    "\n",
    "    fig = plt.plot(h_range, h_score)\n",
    "\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_h = h_range[np.argmax(h_score)]\n",
    "    return optimal_h\n",
    "\n",
    "def get_cluster_assignments_hac(linkage_matrix, height):\n",
    "    \"Use Hierarchical Agglomerative Clustering to cluster phrase vectors\"\n",
    "    return [x[0] for x in cut_tree(linkage_matrix, height=height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def visualize_umap(just_phrase_ids, cluster_assignments, just_phrase_text, dist, umap_neighbors = 15, save_chart = False, chart_file = \"UMAP.html\"):\n",
    "    \"Visualize the clusters using UMAP\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42)\n",
    "    embedding = reducer.fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, UMAP_cluster=cluster_assignments, phrase=just_phrase_text, x=embedding[:, 0], y=embedding[:, 1]))\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"UMAP_cluster\", hover_data=[\"phrase\", \"label\",\"UMAP_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_mds(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"MDS.html\"):\n",
    "    \"Visualize the clusters using Multi-Dimensional Scaling (MDS)\"\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "    pos = mds.fit_transform(dist)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, MDS_cluster=cluster_assignments, label=just_phrase_ids, phrase=just_phrase_text)) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"MDS_cluster\", hover_data=[\"phrase\", \"label\",\"MDS_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_svd(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"SVD.html\"):\n",
    "    \"Visualize the clusters using Singular Value Decomposition (SVD)\"\n",
    "    svd2d =  TruncatedSVD(n_components = 2, random_state = 42).fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, SVD_cluster=cluster_assignments, phrase=just_phrase_text, x=svd2d[:, 0], y=svd2d[:, 1])) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"SVD_cluster\", hover_data=[\"phrase\", \"label\",\"SVD_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cluster_topics(cluster_assignments, just_phrase_ids, my_docs):\n",
    "    \"Gets a list of the main topics for each cluster\"\n",
    "    cluster_topics = []\n",
    "    for c in set(cluster_assignments):\n",
    "        sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)\n",
    "\n",
    "        ## sent_coords has each element formatted as [doc number, sentence number]\n",
    "        cluster_terms = []\n",
    "        for doc_id, sent_id in sent_coords:\n",
    "            sent = my_docs[doc_id][sent_id]\n",
    "            # This isn't used. Can we remove?\n",
    "#             pos = my_docs_pos[doc_id][sent_id] \n",
    "            cluster_terms.append(sent)\n",
    "\n",
    "        cluster_topics.append(cluster_terms)\n",
    "        \n",
    "    main_cluster_topics = []\n",
    "    for doc_topics in cluster_topics:\n",
    "        #Latent Dirichlet Allocation implementation with Gensim\n",
    "        dictionary = corpora.Dictionary(doc_topics)\n",
    "        corpus = [dictionary.doc2bow(text) for text in doc_topics]\n",
    "        lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "        topics_matrix = lda.show_topics(formatted=False, num_words=10)\n",
    "\n",
    "        topics_ary = list(np.array(topics_matrix[0][1])[:,0])\n",
    "        main_cluster_topics.append(topics_ary)\n",
    "    return main_cluster_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_output_to_disk(main_cluster_topics, just_phrase_ids, cluster_assignments, raw_sentences, \n",
    "                         doc_top_phrases, file_list, file_name = \"TopicClusterResults.txt\"):\n",
    "    \"Creates a CSV file listing cluster_id/doc_id/doc_name/sentence_id/sentence_text/sentences_phrases\"\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        for c, topics in enumerate(main_cluster_topics):\n",
    "            outfile.write(\"Cluster\\tDocument.Num\\tDocument.Name\\tSentence.Num\\tSentence.Text\\nPhrase.Text\")\n",
    "            sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)\n",
    "            outfile.write(f\"Cluster {c} Keywords: {', '.join(topics)}\\n\")\n",
    "\n",
    "            ## sent_coords has each element formatted as [doc number, sentence number]\n",
    "            for doc_id, sent_id in sent_coords:\n",
    "                if doc_id >= 0 and sent_id >= 0:\n",
    "                    outfile.write(f\"{c}\\t{doc_id}\\t{file_list[doc_id]}\\t{sent_id}\\t{raw_sentences[doc_id][sent_id]}\\t{doc_top_phrases[doc_id][sent_id]}\\n\")\n",
    "                else:\n",
    "                    #QUESTION: What are we trying to do here? When would this be hit?\n",
    "                    outfile.write(f\"{c}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(just_phrase_ids, cluster_assignments, just_phrase_text, \n",
    "             gold_file = \"data/GOLD_Expert_2019.03.12_TestCorpus_AMY.txt\", output_file = \"output/EvaluationResults.txt\"):\n",
    "    \"Evaluate precision, recall, and F1 against a gold standard dataset. Write results to a file\"\n",
    "    \n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, cluster=cluster_assignments, phrase=just_phrase_text))\n",
    "    \n",
    "    # Process the gold standard file into a list of IDs and classification groups\n",
    "    with open(gold_file) as file:\n",
    "        gold_list = file.read().strip().split('\\n')\n",
    "    gold_array = np.array([x.split('\\t') for x in gold_list])\n",
    "    ids, group = gold_array.T    \n",
    "    baseline = list(np.unique(group)) # This is the list of classes ['Confidence', 'Overwhelmed', 'Supportive Environment', 'System Issues']\n",
    "\n",
    "    # Dictionary of IDs corresponding to each class\n",
    "    gold_clusters = {}\n",
    "    for clazz in baseline:\n",
    "        # List of IDs (doc.#.sent.#) corresponding to a given class\n",
    "        gold_clusters[clazz] = gold_array[gold_array[:,1] == clazz][:,0]\n",
    "\n",
    "    #TODO: This can be combined with the for loop above\n",
    "    closest_to_gold = {}\n",
    "    for b in baseline:\n",
    "        max_overlap = 0\n",
    "        max_overlap_cluster = -1\n",
    "        cluster_ids = list(set(cluster_assignments))\n",
    "        just_phrase_ids\n",
    "\n",
    "        for c in cluster_ids:\n",
    "            overlap = len(set(gold_clusters[b]).intersection(set(df.label[df.cluster==c])))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                max_overlap_cluster = c\n",
    "        closest_to_gold[b] = max_overlap_cluster\n",
    "\n",
    "    #TODO: I think this could also be combined with the above FOR loop\n",
    "    # Write results to a file\n",
    "    with open(output_file, \"w\") as eout:\n",
    "        eout.write(\"Gold Concept\\tGold Concept Members\\tClosest Cluster Num\\tClosest Cluster Members\\tTP\\tFP\\tFN\\tP\\tR\\tF1\\n\")\n",
    "        for g in gold_clusters.keys():\n",
    "            ## We only want to do these calculations on sentences that are in the gold file.\n",
    "            ## Get closest cluster list of sentences that are ONLY in gold\n",
    "            closest = set(df.label[df.cluster==closest_to_gold[g]]).intersection(set(ids))\n",
    "            gold = set(gold_clusters[g])\n",
    "            TP = len(gold.intersection(closest))\n",
    "            FP = len(closest - gold)\n",
    "            FN = len(gold - closest)\n",
    "            P = round(TP/(TP+FP), 3) if (TP+FP) > 0 else float(\"Nan\")\n",
    "            R = round(TP/(TP+FN), 3) if (TP+FN) > 0 else float(\"Nan\")\n",
    "            F1 = round(2*((P*R)/(P+R)), 3) if (P+R) > 0 else float(\"Nan\")\n",
    "            eout.write(f\"{g}\\t{gold}\\t{closest_to_gold[g]}\\t{closest}\\t{TP}\\t{FP}\\t{FN}\\t{P}\\t{R}\\t{F1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted helpers.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nlp_helpers.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
