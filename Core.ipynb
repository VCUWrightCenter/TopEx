{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "from medtop.nlp_helpers import *\n",
    "from medtop.preprocessing import *\n",
    "import numpy\n",
    "from scipy.cluster.hierarchy import ward, cut_tree\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Methods for calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_docs(path_to_file_list, verbose = False):\n",
    "    \"Imports and processes the list of documents contained in the given file\"\n",
    "    \n",
    "    # Extract list of files from text document\n",
    "    with open(path_to_file_list) as file:\n",
    "        file_list = file.read().strip().split('\\n')\n",
    "    \n",
    "    # Append the raw contents of each document to an array\n",
    "    file_content = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            file_content.append(text)\n",
    "        \n",
    "    my_docs = []\n",
    "    my_docs_pos = []\n",
    "    my_docs_loc = []    \n",
    "    raw_docs = []\n",
    "    raw_sentences = []\n",
    "\n",
    "    for doc in file_content:\n",
    "        split_sent, split_sent_pos, split_sent_loc, tokens, pos, loc, full_sent, full_pos, full_loc, raw_sent = tokenize_and_stem(doc)\n",
    "        my_docs.append(split_sent)\n",
    "        my_docs_pos.append(split_sent_pos)\n",
    "        my_docs_loc.append(split_sent_loc)\n",
    "        raw_docs.append(doc)\n",
    "        raw_sentences.append(raw_sent)\n",
    "        \n",
    "    if verbose == True: print(\"Number of Documents Loaded: \" + str(len(my_docs)))\n",
    "        \n",
    "    return my_docs, my_docs_pos, my_docs_loc, raw_sentences, raw_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sentences_to_disk(raw_sentences, outfile_name):\n",
    "    \"Writes the raw sentences to a file organized by document and sentence number\"\n",
    "#     outfile_name = args.o + \"/\" + args.p + \"_DocumentSentenceList.txt\"\n",
    "    with open(outfile_name, \"w\") as outfile:\n",
    "        outfile.write(\"Sentence ID\\tSentence Text\\n\")\n",
    "        for doc_ix, doc in enumerate(raw_sentences):\n",
    "            for sent_ix, sent in enumerate(doc):\n",
    "                outfile.write(f\"doc.{doc_ix}.sent.{sent_ix}\\t{sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_top_phrases(raw_docs, feature_names, tdm, window_size = 6, include_input_in_tfidf = False):\n",
    "#     \"\"\"\n",
    "#     feature_names = dictionary.token2id\n",
    "#     tdm = tfidf_dense\n",
    "#     \"\"\"\n",
    "    #TODO: Do this without reparsing each sentence in get_top_phrases\n",
    "    doc_top_phrases = []    \n",
    "    for doc_id, text in enumerate(raw_docs):\n",
    "        top_phrases = get_top_phrases(doc_id, text, feature_names, tdm, window_size, include_input_in_tfidf)\n",
    "        doc_top_phrases.append(top_phrases)\n",
    "        \n",
    "    return doc_top_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_word_vectors_tfidf(doc_top_phrases, dictionary, tfidf_dense):\n",
    "    \"Uses the TF-IDF matrix to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_dense)\n",
    "\n",
    "def get_doc_word_vectors_svd(doc_top_phrases, dictionary, tfidf_dense, dimensions = 2):\n",
    "    \"Decomposes the TF-IDF matrix via SVD and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    svd =  TruncatedSVD(n_components = dimensions, random_state = 42)\n",
    "    tfidf_transf = svd.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_transf)\n",
    "\n",
    "def get_doc_word_vectors_umap(doc_top_phrases, dictionary, tfidf_dense, umap_neighbors = 15, dimensions = 2):\n",
    "    \"Transforms the TF-IDF matrix via UMAP and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components = dimensions)\n",
    "    embed = reducer.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, embed)\n",
    "\n",
    "def get_doc_word_vectors_pretrained(doc_top_phrases, path_to_w2v_bin_file):\n",
    "    \"Uses pretrained Word2Vec embeddings to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    assert os.path.exists(path_to_w2v_bin_file), f'The file {path_to_w2v_bin_file} does not exist'\n",
    "    model = w2v_pretrained(path_to_w2v_bin_file)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)\n",
    "\n",
    "def get_doc_word_vectors_local(doc_top_phrases, raw_docs, my_docs):\n",
    "    \"Creates Word2Vec embeddings from the input corpus and uses them to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    model = w2v_from_corpus(raw_docs, my_docs)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_sentences(doc_phrase_vecs, doc_top_phrases):\n",
    "    \"Filter the sentences, removing any that contain zero 'top phrases'\"\n",
    "    just_phrase_vecs = []\n",
    "    just_phrase_ids = []\n",
    "    just_phrase_text = []\n",
    "\n",
    "    # Getting all phrases which have \n",
    "    # Iterate documents\n",
    "    for doc_ix, phrase_vecs in enumerate(doc_phrase_vecs):\n",
    "        # Iterate sentences (each sentence has a phrase vector)\n",
    "        for phrase_ix, phrase_vec in enumerate(phrase_vecs):\n",
    "            phrase_id = f\"doc.{doc_ix}.sent.{phrase_ix}\"\n",
    "            this_text = doc_top_phrases[doc_ix][phrase_ix]\n",
    "\n",
    "            # Check phrase_vec is not zero or an array of zeros, these would imply the sentence has no \"top phrases\"\n",
    "            if isinstance(phrase_vec, numpy.ndarray) and any(numpy.zeros(len(phrase_vec)) != phrase_vec):\n",
    "                just_phrase_vecs.append(list(phrase_vec))\n",
    "                just_phrase_ids.append(phrase_id)\n",
    "                just_phrase_text.append(this_text[1])\n",
    "    return just_phrase_vecs, just_phrase_ids, just_phrase_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_optimal_k(just_phrase_vecs, k_range = range(2, 100), save_chart = True, chart_title = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors for a range of k values and plots the silhoute coefficients \n",
    "    (this is a function of mean intra-cluster distance and mean nearest-cluster distance). \n",
    "    Returns the optimal k-value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    score = [(silhouette_score(just_phrase_vecs, KMeans(i).fit(just_phrase_vecs).predict(just_phrase_vecs))) for i in k_range]\n",
    "    fig = plt.plot(k_range, score)\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_title, dpi=300)\n",
    "    optimal_k = k_range[score.index(max(score))]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(k, just_phrase_vecs):\n",
    "    \"Use K-means algorithm to cluster phr\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(just_phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(just_phrase_vecs)\n",
    "    #TODO: this is never used before being overwritten later. Remove?\n",
    "    dist = pairwise_distances(just_phrase_vecs, metric='euclidean') \n",
    "    return cluster_assignments, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted helpers.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nlp_helpers.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
