{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gensim\n",
    "from gensim import corpora, models, matutils\n",
    "import matplotlib.pyplot as plt\n",
    "import medtop.internal as internal\n",
    "import medtop.preprocessing as preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> MedTop core API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_docs(path_to_file_list:str, save_results:bool = False, file_name:str = 'output/DocumentSentenceList.txt',\n",
    "               stop_words_file:str = None):\n",
    "    \"\"\"\n",
    "    Imports and pre-processes the list of documents contained in the input file.\n",
    "    \n",
    "    Document pre-processing is handled in [`tokenize_and_stem`](/medtop/preprocessing#tokenize_and_stem).\n",
    "    `path_to_file_list` is a path to a text file containing a list of files to be processed separated by line breaks.\n",
    "    \n",
    "    Returns (DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract list of files from text document\n",
    "    with open(path_to_file_list, encoding=\"utf-8\") as file:\n",
    "        file_list = file.read().strip().split('\\n')\n",
    "    \n",
    "    # Create a dataframe containing the doc_id, file name, and raw text of each document\n",
    "    doc_cols = [\"doc_id\", \"file\", \"text\", \"tokens\"]\n",
    "    doc_rows = []\n",
    "    \n",
    "    # Create a dataframe containing the id, doc_id, sent_id, raw text, tokens, and PoS tags for each sentence\n",
    "    data_cols = [\"id\", \"doc_id\", \"sent_id\", \"text\", \"tokens\", \"pos_tags\"]    \n",
    "    data_rows = []\n",
    "    \n",
    "    for doc_id, file in enumerate(file_list):\n",
    "        # Read document\n",
    "        with open(file, encoding=\"utf-8\") as file_content:\n",
    "            doc_text = file_content.read()\n",
    "        \n",
    "        # Pre-process document into cleaned sentences\n",
    "        sent_tokens, sent_pos, raw_sent = preprocessing.tokenize_and_stem(doc_text, stop_words_file=stop_words_file)\n",
    "\n",
    "        # Populate a row in data for each sentence in the document\n",
    "        for sent_id, _ in enumerate(sent_tokens):\n",
    "            row_id = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            sent_text = raw_sent[sent_id]\n",
    "            tokens = sent_tokens[sent_id]\n",
    "            pos_tags = sent_pos[sent_id]\n",
    "            data_rows.append(Series([row_id, doc_id, sent_id, sent_text, tokens, pos_tags], index=data_cols))\n",
    "        \n",
    "        # Populate a row in doc_df for each file loaded\n",
    "        doc_tokens = [token for sent in sent_tokens for token in sent]\n",
    "        doc_row = Series([doc_id, file, doc_text, doc_tokens], index=doc_cols)\n",
    "        doc_rows.append(doc_row)\n",
    "    \n",
    "    # Create dataframes from lists of Series\n",
    "    data = DataFrame(data_rows)\n",
    "    doc_df = DataFrame(doc_rows)\n",
    "    \n",
    "    # Optionally save the results to disk\n",
    "    if save_results:\n",
    "        internal.sentences_to_disk(data, file_name)\n",
    "\n",
    "    return data, doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tfidf(doc_df:DataFrame = None, path_to_seed_topics_file_list:str = None):\n",
    "    \"\"\"\n",
    "    Creates a dense TF-IDF matrix from the tokens in the seed topics documents and/or the input corpus.\n",
    "    \n",
    "    `path_to_seed_topics_file_list` is a path to a text file containing a list of files with sentences corresponding to \n",
    "    known topics. If the `doc_df` is passed, the input corpus will be used along with the seed topics documents to\n",
    "    generate the TF-IDF matrix.\n",
    "    \n",
    "    Returns (numpy.ndarray, gensim.corpora.dictionary.Dictionary)\n",
    "    \"\"\"\n",
    "    # Bag of Words (BoW) is a list of all tokens by document\n",
    "    bow_docs = []\n",
    "    \n",
    "    if doc_df is not None:\n",
    "        bow_docs = bow_docs + list(doc_df.tokens)\n",
    "        \n",
    "    # Import seed topics documents\n",
    "    if path_to_seed_topics_file_list is not None:\n",
    "        _, seed_doc_df = import_docs(path_to_seed_topics_file_list)\n",
    "        bow_docs = bow_docs + list(seed_doc_df.tokens)    \n",
    "\n",
    "    # Create a dense TF-IDF matrix using document tokens and gensim\n",
    "    dictionary = corpora.Dictionary(bow_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in bow_docs]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    num_terms = len(corpus_tfidf.obj.idfs)\n",
    "    tfidf_dense = matutils.corpus2dense(corpus_tfidf, num_terms)\n",
    "    \n",
    "    return tfidf_dense, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "def get_phrases(data:DataFrame, feature_names:dict, tdm:np.ndarray, window_size:int = 6, \n",
    "                include_input_in_tfidf:bool = True):\n",
    "    \"\"\"\n",
    "    Extracts the most expressive phrase from each sentence.\n",
    "    \n",
    "    `feature_names` should be `dictionary.token2id` and `tdm` should be `tfidf` where `dictionary`\n",
    "    and `tfidf` are output from `create_tfidf`. `window_size` is the length of phrase extracted, if a -1 is passed,\n",
    "    all tokens will be included (IMPORTANT: this option requires aggregating vectors in the next step. \n",
    "    When `include_input_in_tfidf` is True, token_scores are calculated using the TF-IDF, otherwise, token_scores \n",
    "    are calculated using `token_averages`.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    if window_size > 0:\n",
    "        token_averages = np.max(tdm, axis=1)\n",
    "\n",
    "        # Find the most expressive phrase for each sentence and add to dataframe\n",
    "        lambda_func = lambda sent: internal.get_phrase(sent, window_size, feature_names, include_input_in_tfidf, tdm, \n",
    "                                                       token_averages)\n",
    "        phrases = data.apply(lambda_func, axis=1)\n",
    "        data['phrase'] = phrases\n",
    "    else:\n",
    "        data['phrase'] = data.tokens\n",
    "    \n",
    "    # Remove records where phrase is None\n",
    "    filtered_df = data[data.phrase.notnull()].copy().reset_index(drop=True)\n",
    "    print(f\"Removed {len(data) - len(filtered_df)} sentences without phrases.\")\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_vectors(method:str, data:DataFrame, dictionary:gensim.corpora.dictionary.Dictionary = None, \n",
    "                tfidf:np.ndarray = None, dimensions:int = 2, umap_neighbors:int = 15, \n",
    "                path_to_w2v_bin_file:str = None, doc_df:DataFrame = None):\n",
    "    \"\"\"\n",
    "    Creates a word vector for each phrase in the dataframe.\n",
    "    \n",
    "    Options for `method` are ('tfidf', 'svd', 'umap', 'pretrained', 'local'). Options for `method` are ('tfidf', 'svd', 'umap', 'pretrained', 'local').`tfidf` and `dictionary` are output from \n",
    "    `create_tfidf`. `dimensions` is the number of dimensions to which SVD or UMAP reduce the TF-IDF matrix. \n",
    "    `path_to_w2v_bin_file` is the path to a pretrained Word2Vec .bin file.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create word vectors with TF-IDF\n",
    "    if method == \"tfidf\":\n",
    "        assert dictionary is not None, \"Optional parameter: 'dictionary' is required for method: 'tfidf'.\"\n",
    "        assert tfidf is not None, \"Optional parameter: 'tfidf' is required for method: 'tfidf'.\"\n",
    "        lambda_func = lambda sent: internal.get_vector_tfidf(sent, dictionary, tfidf)\n",
    "        \n",
    "    # Create word vectors with SVD transformed TF-IDF\n",
    "    elif method == \"svd\":\n",
    "        assert dictionary is not None, \"Optional parameter: 'dictionary' is required for method: 'svd'.\"\n",
    "        assert tfidf is not None, \"Optional parameter: 'tfidf' is required for method: 'svd'.\"\n",
    "        svd =  TruncatedSVD(n_components = dimensions, random_state = 42)\n",
    "        tfidf_transf = svd.fit_transform(tfidf)\n",
    "        lambda_func = lambda sent: internal.get_vector_tfidf(sent, dictionary, tfidf_transf)\n",
    "        \n",
    "    # Create word vectors with UMAP transformed TF-IDF\n",
    "    elif method == \"umap\":\n",
    "        assert dictionary is not None, \"Optional parameter: 'dictionary' is required for method: 'umap'.\"\n",
    "        assert tfidf is not None, \"Optional parameter: 'tfidf' is required for method: 'umap'.\"\n",
    "        reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components=dimensions)\n",
    "        embed = reducer.fit_transform(tfidf)\n",
    "        lambda_func = lambda sent: internal.get_vector_tfidf(sent, dictionary, embed)\n",
    "    \n",
    "    # Create word vectors using a pre-trained Word2Vec model\n",
    "    elif method == \"pretrained\":\n",
    "        assert path_to_w2v_bin_file is not None, \"Optional parameter: 'path_to_w2v_bin_file' is required for method: 'pretrained'.\"\n",
    "        model = internal.w2v_pretrained(path_to_w2v_bin_file)\n",
    "        lambda_func = lambda sent: internal.get_vector_w2v(sent, model)\n",
    "        \n",
    "    # Generate a Word2Vec model from the input corpus and use it to create word vectors for each phrase\n",
    "    elif method == \"local\":\n",
    "        assert doc_df is not None, \"Optional parameter: 'doc_df' is required for method: 'local'.\"\n",
    "        large_sent_vec = list(doc_df.tokens)\n",
    "        model = models.Word2Vec(sg=1, window = 6, max_vocab_size = None, min_count=1, size=10, iter=500)\n",
    "        model.build_vocab(large_sent_vec)\n",
    "        model.train(large_sent_vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        lambda_func = lambda sent: internal.get_vector_w2v(sent, model)\n",
    "        \n",
    "    # Invalid input paramter\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized method: '{method}'\")\n",
    "        \n",
    "    # Create a word vector for each phrase and append it to the dataframe\n",
    "    vectors = data.apply(lambda_func, axis=1)\n",
    "    data['vec'] = vectors\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def assign_clusters(data:DataFrame, method:str = \"kmeans\", dist_metric:str = \"euclidean\", k:int = None, \n",
    "                    height:int = None, show_chart:bool = False, show_dendrogram:bool = False):\n",
    "    \"\"\"\n",
    "    Clusters the sentences using phrase vectors.\n",
    "    \n",
    "    Options for `method` are ('kmeans', 'hac'). Options for `dist_metric` are ('cosine' or anything accepted by \n",
    "    sklearn.metrics.pairwise_distances). `k` is the number of clusters for K-means clustering. `height` is the height \n",
    "    at which the HAC dendrogram should be cut. When `show_chart` is True, the chart of silhoute scores by possible k or \n",
    "    height is shown inline. When `show_dendrogram` is True, the HAC dendrogram is shown inline.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cluster using K-means algorithm\n",
    "    if method == \"kmeans\":\n",
    "        cluster_assignments = internal.get_clusters_kmeans(data, k, show_chart = show_chart)\n",
    "        \n",
    "    # Cluster using Hierarchical Agglomerative Clustering (HAC)\n",
    "    elif method == \"hac\":\n",
    "        cluster_assignments = internal.get_clusters_hac(data, dist_metric = dist_metric, height = height, \n",
    "                                                        show_chart = show_chart, show_dendrogram = show_dendrogram)\n",
    "        \n",
    "    # Invalid input parameter\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized method: '{method}'\")\n",
    "    \n",
    "    data['cluster'] = cluster_assignments\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ca44de0aebab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m def visualize_clustering(data:DataFrame, method:str = \"umap\", dist_metric:str = \"cosine\", umap_neighbors:int = 15, \n\u001b[1;32m----> 3\u001b[1;33m                          show_chart = True, save_chart = False, chart_file = \"cluster_visualization.html\"):\n\u001b[0m\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m     \u001b[0mVisualize\u001b[0m \u001b[0mclustering\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "#export\n",
    "def visualize_clustering(data:DataFrame, method:str = \"umap\", dist_metric:str = \"cosine\", umap_neighbors:int = 15, \n",
    "                         show_chart = True, save_chart = False, return_data = False, chart_file = \"cluster_visualization.html\"):\n",
    "    \"\"\"\n",
    "    Visualize clustering in two dimensions.\n",
    "    \n",
    "    Options for `method` are ('umap', 'mds', 'svd'). Options for `dist_metric` are ('cosine' or anything accepted by \n",
    "    sklearn.metrics.pairwise_distances). When `show_chart` is True, the visualization is shown inline. \n",
    "    When `save_chart` is True, the visualization is saved to `chart_file`.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate distances between all pairs of phrases\n",
    "    dist = pairwise_distances(list(data.vec), metric=dist_metric)\n",
    "    \n",
    "    # Visualize the clusters using UMAP\n",
    "    if method == \"umap\":\n",
    "        reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42)\n",
    "        embedding = reducer.fit_transform(dist)\n",
    "        x, y = embedding[:, 0], embedding[:, 1]\n",
    "    \n",
    "    # Visualize the clusters using Multi-Dimensional Scaling (MDS)\n",
    "    elif method == \"mds\":\n",
    "        mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "        pos = mds.fit_transform(dist)\n",
    "        x, y = pos[:, 0], pos[:, 1]      \n",
    "        \n",
    "    # Visualize the clusters using Singular Value Decomposition (SVD)\n",
    "    elif method == \"svd\":\n",
    "        svd2d =  TruncatedSVD(n_components = 2, random_state = 42).fit_transform(dist)\n",
    "        x, y = svd2d[:, 0], svd2d[:, 1]    \n",
    "        \n",
    "    # Invalid input parameter\n",
    "    else:\n",
    "        raise Exception(f\"Unrecognized method: '{method}'\")\n",
    "    \n",
    "    # Print visualization to screen by default\n",
    "    if show_chart:\n",
    "        visualization_df = DataFrame(dict(id=list(data.id), cluster=list(data.cluster), phrase=list(data.phrase), x=x, y=y))\n",
    "        fig = px.scatter(visualization_df, x=\"x\", y=\"y\", hover_name=\"id\", color=\"cluster\", hover_data=[\"phrase\",\"cluster\"], \n",
    "                         color_continuous_scale='rainbow')\n",
    "        fig.show()\n",
    "    \n",
    "    # Optionally save the chart to a file\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "    \n",
    "    # Return the data to display clusters\n",
    "    if return_data:\n",
    "        return pd.DataFrame(dict(label=data.id, cluster=data.cluster, phrase=data.phrase, text=data.text, x=x, y=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cluster_topics(data:DataFrame, doc_df:DataFrame = None, topics_per_cluster:int = 10, save_results:bool = False, \n",
    "                       file_name:str = 'output/TopicClusterResults.txt'):\n",
    "    \"\"\"\n",
    "    Gets the main topics for each cluster.\n",
    "    \n",
    "    `topics_per_cluster` is the number of main topics per cluster. When `save_results` is True, the resulting dataframe \n",
    "    will be saved to `file_name`.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate distinct clusters\n",
    "    rows = []\n",
    "    for c in set(data.cluster):\n",
    "        cluster_tokens = [t for t in data[data.cluster == c].tokens]\n",
    "        topics = internal.get_topics_from_docs(cluster_tokens, topics_per_cluster)\n",
    "        rows.append(Series([c, topics, len(cluster_tokens)], index=[\"cluster\", \"topics\", \"sent_count\"]))\n",
    "    \n",
    "    cluster_df = DataFrame(rows)\n",
    "    \n",
    "    # Optionally save clusters to disk\n",
    "    if save_results:\n",
    "        assert doc_df is not None, \"Optional parameter: 'doc_df' is required when save_results = True.\"\n",
    "        internal.clusters_to_disk(data, doc_df, cluster_df, file_name)\n",
    "                         \n",
    "    return cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_topics(doc_df:DataFrame, topics_per_doc:int = 10, save_results:bool = False, \n",
    "                       file_name:str = 'output/TopicDocumentResults.txt'):\n",
    "    \"\"\"\n",
    "    Gets the main topics for each document.\n",
    "    \n",
    "    `topics_per_doc` is the number of topics extracted per document. When `save_results` is True, the resulting dataframe \n",
    "    will be saved to `file_name`.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    doc_df['topics'] = [internal.get_topics_from_docs([doc], topics_per_doc) for doc in doc_df.tokens]\n",
    "    \n",
    "    # Optionally save clusters to disk\n",
    "    if save_results:\n",
    "        internal.df_to_disk(doc_df, file_name)\n",
    "                         \n",
    "    return doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(data, gold_file, save_results = False, file_name = \"output/EvaluationResults.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate precision, recall, and F1 against a gold standard dataset.\n",
    "    \n",
    "    `gold_file` is a path to a text file containing a list IDs and labels. When `save_results` is True, the resulting \n",
    "    dataframe will be saved to `file_name`.\n",
    "\n",
    "    Returns DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import gold standard list of IDs (doc.#.sent.#) and labels\n",
    "    gold_df = pd.read_csv(gold_file, names=[\"id\", \"label\"], sep=\"\\t\", encoding=\"utf-8\")\n",
    "    \n",
    "    # Inner join the actual labels with the assigned clusters for each document.\n",
    "    eval_df = pd.merge(gold_df, data[[\"id\", \"cluster\"]], on=\"id\")\n",
    "\n",
    "    # Iterate labels in the gold standard dataset\n",
    "    rows = []\n",
    "    for label in set(gold_df.label):\n",
    "        cluster_rows = eval_df[eval_df.label == label]\n",
    "\n",
    "        # Find the cluster with the most instances of each label in the gold standard dataset\n",
    "        closest_cluster = cluster_rows.cluster.mode()[0] if len(cluster_rows) > 0 else -1\n",
    "        \n",
    "        # IDs assigned to a label in the gold standard dataset\n",
    "        gold_examples = set(gold_df[gold_df.label == label].id)\n",
    "        \n",
    "        # IDs in the closest cluster\n",
    "        closest_cluster_members = set(eval_df[eval_df.cluster == closest_cluster].id)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        tp = len(gold_examples.intersection(closest_cluster_members))\n",
    "        fp = len(closest_cluster_members - gold_examples)\n",
    "        fn = len(gold_examples - closest_cluster_members)\n",
    "        precision = round(tp/(tp+fp), 3) if (tp+fp) > 0 else float(\"Nan\")\n",
    "        recall = round(tp/(tp+fn), 3) if (tp+fn) > 0 else float(\"Nan\")\n",
    "        f1 = round(2*((precision*recall)/(precision+recall)), 3) if (precision+recall) > 0 else float(\"Nan\")\n",
    "\n",
    "        rows.append(Series([label, gold_examples, closest_cluster, closest_cluster_members, tp, fp, fn, precision, recall, f1], \n",
    "                              index=[\"label\", \"gold_examples\", \"closest_cluster\", \"closest_cluster_members\", \"tp\", \"fp\", \"fn\", \"precision\", \"recall\", \"f1\"]))\n",
    "    \n",
    "    # Create results dataframe with a row for each label\n",
    "    results_df = DataFrame(rows).sort_values(by=[\"label\"])\n",
    "    \n",
    "    # Optionally save the results to disk\n",
    "    if save_results:\n",
    "        internal.df_to_disk(results_df, file_name)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
