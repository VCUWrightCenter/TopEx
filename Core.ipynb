{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "from gensim import corpora, models, matutils\n",
    "import matplotlib.pyplot as plt\n",
    "from medtop.helpers import *\n",
    "from medtop.nlp_helpers import *\n",
    "from medtop.preprocessing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import ward, cut_tree\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Methods intended to be called publicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_docs(path_to_file_list):\n",
    "    \"Imports and processes the list of documents contained in the given file\"\n",
    "    \n",
    "    # Extract list of files from text document\n",
    "    with open(path_to_file_list) as file:\n",
    "        file_list = file.read().strip().split('\\n')\n",
    "    \n",
    "    # Create a dataframe containing the doc_id, file name, and raw text of each document\n",
    "    doc_cols = [\"doc_id\", \"file\", \"text\", \"tokens\"]\n",
    "    doc_df = pd.DataFrame(columns=doc_cols)\n",
    "    \n",
    "    # Create a dataframe containing the doc_sent_id, doc_id, sent_id, raw text, and list of words for each sentence\n",
    "    sent_cols = [\"doc_sent_id\", \"doc_id\", \"sent_id\", \"text\", \"tokens\"]\n",
    "    data = pd.DataFrame(columns=sent_cols)\n",
    "    \n",
    "    for doc_id, file in enumerate(file_list):\n",
    "        with open(file, \"r\") as file_content:\n",
    "            doc_text = file_content.read()\n",
    "            \n",
    "        split_sent, raw_sent = tokenize_and_stem(doc_text)\n",
    "        for sent_id, _ in enumerate(split_sent):\n",
    "            # Populate a row in data for each sentence in the document\n",
    "            doc_sent_id = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            sent_text = raw_sent[sent_id]\n",
    "            tokens = split_sent[sent_id]\n",
    "            sent_row = pd.Series([doc_sent_id, doc_id, sent_id, sent_text, tokens], index=sent_cols)\n",
    "            data = data.append(sent_row, ignore_index=True)\n",
    "        \n",
    "        # Populate a row in doc_df for each file loaded\n",
    "        doc_tokens = [token for sent in data[data.doc_id==doc_id].tokens for token in sent]\n",
    "        doc_row = pd.Series([doc_id, file, doc_text, doc_tokens], index=doc_cols)\n",
    "        doc_df = doc_df.append(doc_row, ignore_index=True)\n",
    "    \n",
    "    return data, doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sentences_to_disk(data, file_name = 'output/DocumentSentenceList.txt'):\n",
    "    \"Writes the raw sentences to a file organized by document and sentence number\"\n",
    "    data.to_csv(file_name, columns=[\"doc_sent_id\", \"text\"], sep='\\t', encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_tfidf(path_to_seed_topics_file_list, input_doc_df = None):\n",
    "    \"Creates a TF-IDF matrix from the tokens in the seed topics documents and optionally, the input corpus\"\n",
    "    \n",
    "    # Import seed topics documents\n",
    "    _, seed_doc_df = import_docs(path_to_seed_topics_file_list)\n",
    "     \n",
    "    # Bag of Words (BoW) is a list of all tokens by document\n",
    "    bow_docs = list(seed_doc_df.tokens)\n",
    "    \n",
    "    if input_doc_df is not None:\n",
    "        bow_docs = bow_docs + list(input_doc_df.tokens)\n",
    "\n",
    "    # Create a TF-IDF matrix using document tokens and gensim\n",
    "    dictionary = corpora.Dictionary(bow_docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in bow_docs]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    num_terms = len(corpus_tfidf.obj.idfs)\n",
    "    tfidf_dense = matutils.corpus2dense(corpus_tfidf, num_terms)\n",
    "    \n",
    "    return tfidf_dense, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_top_phrases(raw_docs, feature_names, tdm, window_size = 6, include_input_in_tfidf = False):\n",
    "    #TODO: Do this without reparsing each sentence in get_top_phrases\n",
    "    doc_top_phrases = []    \n",
    "    for doc_id, text in enumerate(raw_docs):\n",
    "        top_phrases = get_top_phrases(doc_id, text, feature_names, tdm, window_size, include_input_in_tfidf)\n",
    "        doc_top_phrases.append(top_phrases)\n",
    "        \n",
    "    return doc_top_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_word_vectors_tfidf(doc_top_phrases, dictionary, tfidf_dense):\n",
    "    \"Uses the TF-IDF matrix to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_dense)\n",
    "\n",
    "def get_doc_word_vectors_svd(doc_top_phrases, dictionary, tfidf_dense, dimensions = 2):\n",
    "    \"Decomposes the TF-IDF matrix via SVD and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    svd =  TruncatedSVD(n_components = dimensions, random_state = 42)\n",
    "    tfidf_transf = svd.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_transf)\n",
    "\n",
    "def get_doc_word_vectors_umap(doc_top_phrases, dictionary, tfidf_dense, umap_neighbors = 15, dimensions = 2):\n",
    "    \"Transforms the TF-IDF matrix via UMAP and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components = dimensions)\n",
    "    embed = reducer.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, embed)\n",
    "\n",
    "def get_doc_word_vectors_pretrained(doc_top_phrases, path_to_w2v_bin_file):\n",
    "    \"Uses pretrained Word2Vec embeddings to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    assert os.path.exists(path_to_w2v_bin_file), f'The file {path_to_w2v_bin_file} does not exist'\n",
    "    model = w2v_pretrained(path_to_w2v_bin_file)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)\n",
    "\n",
    "def get_doc_word_vectors_local(doc_top_phrases, raw_docs, my_docs):\n",
    "    \"Creates Word2Vec embeddings from the input corpus and uses them to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    model = w2v_from_corpus(raw_docs, my_docs)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_sentences(doc_phrase_vecs, doc_top_phrases):\n",
    "    \"Filter the sentences, removing any that contain zero 'top phrases'\"\n",
    "    just_phrase_vecs = []\n",
    "    just_phrase_ids = []\n",
    "    just_phrase_text = []\n",
    "\n",
    "    # Getting all phrases which have \n",
    "    # Iterate documents\n",
    "    for doc_ix, phrase_vecs in enumerate(doc_phrase_vecs):\n",
    "        # Iterate sentences (each sentence has a phrase vector)\n",
    "        for phrase_ix, phrase_vec in enumerate(phrase_vecs):\n",
    "            phrase_id = f\"doc.{doc_ix}.sent.{phrase_ix}\"\n",
    "            this_text = doc_top_phrases[doc_ix][phrase_ix]\n",
    "\n",
    "            # Check phrase_vec is not zero or an array of zeros, these would imply the sentence has no \"top phrases\"\n",
    "            if isinstance(phrase_vec, np.ndarray) and any(np.zeros(len(phrase_vec)) != phrase_vec):\n",
    "                just_phrase_vecs.append(list(phrase_vec))\n",
    "                just_phrase_ids.append(phrase_id)\n",
    "                just_phrase_text.append(this_text[1])\n",
    "    return just_phrase_vecs, just_phrase_ids, just_phrase_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_optimal_k(just_phrase_vecs, save_chart = False, chart_file = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors for a range of k values and plots the silhoute coefficients \n",
    "    (this is a function of mean intra-cluster distance and mean nearest-cluster distance). \n",
    "    Returns the optimal k-value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_k = min(len(just_phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [(silhouette_score(just_phrase_vecs, KMeans(i).fit(just_phrase_vecs).predict(just_phrase_vecs))) for i in k_range]\n",
    "    fig = plt.plot(k_range, score)\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(k, just_phrase_vecs):\n",
    "    \"Use K-means algorithm to cluster phrase vectors\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(just_phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(just_phrase_vecs)\n",
    "    #TODO: this is never used before being overwritten later. Remove?\n",
    "    dist = pairwise_distances(just_phrase_vecs, metric='euclidean') \n",
    "    return cluster_assignments, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_linkage_matrix(just_phrase_vecs, dist_metric):\n",
    "    \"Creates a linkage matrix by calculating distance between phrase vectors\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(just_phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(just_phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(just_phrase_vecs, linkage_matrix, save_chart = False, chart_file = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_h = min(len(just_phrase_vecs), 100)\n",
    "    # QUESTION: What is the limiting factor for the min/max in this range?\n",
    "#     h_range = range(30, max_h) \n",
    "    h_range = range(1,10) \n",
    "    h_score = [(silhouette_score(just_phrase_vecs, [x[0] for x in cut_tree(linkage_matrix, height=i)] )) for i in h_range]\n",
    "\n",
    "    fig = plt.plot(h_range, h_score)\n",
    "\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_h = h_range[np.argmax(h_score)]\n",
    "    return optimal_h\n",
    "\n",
    "def get_cluster_assignments_hac(linkage_matrix, height):\n",
    "    \"Use Hierarchical Agglomerative Clustering to cluster phrase vectors\"\n",
    "    return [x[0] for x in cut_tree(linkage_matrix, height=height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def visualize_umap(just_phrase_ids, cluster_assignments, just_phrase_text, dist, umap_neighbors = 15, save_chart = False, chart_file = \"UMAP.html\"):\n",
    "    \"Visualize the clusters using UMAP\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42)\n",
    "    embedding = reducer.fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, UMAP_cluster=cluster_assignments, phrase=just_phrase_text, x=embedding[:, 0], y=embedding[:, 1]))\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"UMAP_cluster\", hover_data=[\"phrase\", \"label\",\"UMAP_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_mds(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"MDS.html\"):\n",
    "    \"Visualize the clusters using Multi-Dimensional Scaling (MDS)\"\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "    pos = mds.fit_transform(dist)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, MDS_cluster=cluster_assignments, label=just_phrase_ids, phrase=just_phrase_text)) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"MDS_cluster\", hover_data=[\"phrase\", \"label\",\"MDS_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_svd(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"SVD.html\"):\n",
    "    \"Visualize the clusters using Singular Value Decomposition (SVD)\"\n",
    "    svd2d =  TruncatedSVD(n_components = 2, random_state = 42).fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, SVD_cluster=cluster_assignments, phrase=just_phrase_text, x=svd2d[:, 0], y=svd2d[:, 1])) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"SVD_cluster\", hover_data=[\"phrase\", \"label\",\"SVD_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cluster_topics(cluster_assignments, just_phrase_ids, my_docs):\n",
    "    \"Gets a list of the main topics for each cluster\"\n",
    "    cluster_topics = []\n",
    "    for c in set(cluster_assignments):\n",
    "        sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)\n",
    "\n",
    "        ## sent_coords has each element formatted as [doc number, sentence number]\n",
    "        cluster_terms = []\n",
    "        for doc_id, sent_id in sent_coords:\n",
    "            sent = my_docs[doc_id][sent_id]\n",
    "            # This isn't used. Can we remove?\n",
    "#             pos = my_docs_pos[doc_id][sent_id] \n",
    "            cluster_terms.append(sent)\n",
    "\n",
    "        cluster_topics.append(cluster_terms)\n",
    "        \n",
    "    main_cluster_topics = []\n",
    "    for doc_topics in cluster_topics:\n",
    "        #Latent Dirichlet Allocation implementation with Gensim\n",
    "        dictionary = corpora.Dictionary(doc_topics)\n",
    "        corpus = [dictionary.doc2bow(text) for text in doc_topics]\n",
    "        lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "        topics_matrix = lda.show_topics(formatted=False, num_words=10)\n",
    "\n",
    "        topics_ary = list(np.array(topics_matrix[0][1])[:,0])\n",
    "        main_cluster_topics.append(topics_ary)\n",
    "    return main_cluster_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_output_to_disk(main_cluster_topics, just_phrase_ids, cluster_assignments, raw_sentences, \n",
    "                         doc_top_phrases, file_list, file_name = \"TopicClusterResults.txt\"):\n",
    "    \"Creates a CSV file listing cluster_id/doc_id/doc_name/sentence_id/sentence_text/sentences_phrases\"\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        for c, topics in enumerate(main_cluster_topics):\n",
    "            outfile.write(\"Cluster\\tDocument.Num\\tDocument.Name\\tSentence.Num\\tSentence.Text\\nPhrase.Text\")\n",
    "            sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)\n",
    "            outfile.write(f\"Cluster {c} Keywords: {', '.join(topics)}\\n\")\n",
    "\n",
    "            ## sent_coords has each element formatted as [doc number, sentence number]\n",
    "            for doc_id, sent_id in sent_coords:\n",
    "                if doc_id >= 0 and sent_id >= 0:\n",
    "                    outfile.write(f\"{c}\\t{doc_id}\\t{file_list[doc_id]}\\t{sent_id}\\t{raw_sentences[doc_id][sent_id]}\\t{doc_top_phrases[doc_id][sent_id]}\\n\")\n",
    "                else:\n",
    "                    #QUESTION: What are we trying to do here? When would this be hit?\n",
    "                    outfile.write(f\"{c}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\t{doc_id, sent_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(just_phrase_ids, cluster_assignments, just_phrase_text, \n",
    "             gold_file = \"data/GOLD_Expert_2019.03.12_TestCorpus_AMY.txt\", output_file = \"output/EvaluationResults.txt\"):\n",
    "    \"Evaluate precision, recall, and F1 against a gold standard dataset. Write results to a file\"\n",
    "    \n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, cluster=cluster_assignments, phrase=just_phrase_text))\n",
    "    \n",
    "    # Process the gold standard file into a list of IDs and classification groups\n",
    "    with open(gold_file) as file:\n",
    "        gold_list = file.read().strip().split('\\n')\n",
    "    gold_array = np.array([x.split('\\t') for x in gold_list])\n",
    "    ids, group = gold_array.T    \n",
    "    baseline = list(np.unique(group)) # This is the list of classes ['Confidence', 'Overwhelmed', 'Supportive Environment', 'System Issues']\n",
    "\n",
    "    # Dictionary of IDs corresponding to each class\n",
    "    gold_clusters = {}\n",
    "    for clazz in baseline:\n",
    "        # List of IDs (doc.#.sent.#) corresponding to a given class\n",
    "        gold_clusters[clazz] = gold_array[gold_array[:,1] == clazz][:,0]\n",
    "\n",
    "    #TODO: This can be combined with the for loop above\n",
    "    closest_to_gold = {}\n",
    "    for b in baseline:\n",
    "        max_overlap = 0\n",
    "        max_overlap_cluster = -1\n",
    "        cluster_ids = list(set(cluster_assignments))\n",
    "        just_phrase_ids\n",
    "\n",
    "        for c in cluster_ids:\n",
    "            overlap = len(set(gold_clusters[b]).intersection(set(df.label[df.cluster==c])))\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                max_overlap_cluster = c\n",
    "        closest_to_gold[b] = max_overlap_cluster\n",
    "\n",
    "    #TODO: I think this could also be combined with the above FOR loop\n",
    "    # Write results to a file\n",
    "    with open(output_file, \"w\") as eout:\n",
    "        eout.write(\"Gold Concept\\tGold Concept Members\\tClosest Cluster Num\\tClosest Cluster Members\\tTP\\tFP\\tFN\\tP\\tR\\tF1\\n\")\n",
    "        for g in gold_clusters.keys():\n",
    "            ## We only want to do these calculations on sentences that are in the gold file.\n",
    "            ## Get closest cluster list of sentences that are ONLY in gold\n",
    "            closest = set(df.label[df.cluster==closest_to_gold[g]]).intersection(set(ids))\n",
    "            gold = set(gold_clusters[g])\n",
    "            TP = len(gold.intersection(closest))\n",
    "            FP = len(closest - gold)\n",
    "            FN = len(gold - closest)\n",
    "            P = round(TP/(TP+FP), 3) if (TP+FP) > 0 else float(\"Nan\")\n",
    "            R = round(TP/(TP+FN), 3) if (TP+FN) > 0 else float(\"Nan\")\n",
    "            F1 = round(2*((P*R)/(P+R)), 3) if (P+R) > 0 else float(\"Nan\")\n",
    "            eout.write(f\"{g}\\t{gold}\\t{closest_to_gold[g]}\\t{closest}\\t{TP}\\t{FP}\\t{FN}\\t{P}\\t{R}\\t{F1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted helpers.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted nlp_helpers.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
