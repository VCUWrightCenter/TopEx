{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "from medtop.nlp_helpers import *\n",
    "from medtop.preprocessing import *\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from scipy.cluster.hierarchy import ward, cut_tree\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Methods for calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_docs(path_to_file_list, verbose = False):\n",
    "    \"Imports and processes the list of documents contained in the given file\"\n",
    "    \n",
    "    # Extract list of files from text document\n",
    "    with open(path_to_file_list) as file:\n",
    "        file_list = file.read().strip().split('\\n')\n",
    "    \n",
    "    # Append the raw contents of each document to an array\n",
    "    file_content = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            file_content.append(text)\n",
    "        \n",
    "    my_docs = []\n",
    "    my_docs_pos = []\n",
    "    my_docs_loc = []    \n",
    "    raw_docs = []\n",
    "    raw_sentences = []\n",
    "\n",
    "    for doc in file_content:\n",
    "        split_sent, split_sent_pos, split_sent_loc, tokens, pos, loc, full_sent, full_pos, full_loc, raw_sent = tokenize_and_stem(doc)\n",
    "        my_docs.append(split_sent)\n",
    "        my_docs_pos.append(split_sent_pos)\n",
    "        my_docs_loc.append(split_sent_loc)\n",
    "        raw_docs.append(doc)\n",
    "        raw_sentences.append(raw_sent)\n",
    "        \n",
    "    if verbose == True: print(\"Number of Documents Loaded: \" + str(len(my_docs)))\n",
    "        \n",
    "    return my_docs, my_docs_pos, my_docs_loc, raw_sentences, raw_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sentences_to_disk(raw_sentences, outfile_name):\n",
    "    \"Writes the raw sentences to a file organized by document and sentence number\"\n",
    "#     outfile_name = args.o + \"/\" + args.p + \"_DocumentSentenceList.txt\"\n",
    "    with open(outfile_name, \"w\") as outfile:\n",
    "        outfile.write(\"Sentence ID\\tSentence Text\\n\")\n",
    "        for doc_ix, doc in enumerate(raw_sentences):\n",
    "            for sent_ix, sent in enumerate(doc):\n",
    "                outfile.write(f\"doc.{doc_ix}.sent.{sent_ix}\\t{sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_top_phrases(raw_docs, feature_names, tdm, window_size = 6, include_input_in_tfidf = False):\n",
    "    #TODO: Do this without reparsing each sentence in get_top_phrases\n",
    "    doc_top_phrases = []    \n",
    "    for doc_id, text in enumerate(raw_docs):\n",
    "        top_phrases = get_top_phrases(doc_id, text, feature_names, tdm, window_size, include_input_in_tfidf)\n",
    "        doc_top_phrases.append(top_phrases)\n",
    "        \n",
    "    return doc_top_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_doc_word_vectors_tfidf(doc_top_phrases, dictionary, tfidf_dense):\n",
    "    \"Uses the TF-IDF matrix to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_dense)\n",
    "\n",
    "def get_doc_word_vectors_svd(doc_top_phrases, dictionary, tfidf_dense, dimensions = 2):\n",
    "    \"Decomposes the TF-IDF matrix via SVD and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    svd =  TruncatedSVD(n_components = dimensions, random_state = 42)\n",
    "    tfidf_transf = svd.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, tfidf_transf)\n",
    "\n",
    "def get_doc_word_vectors_umap(doc_top_phrases, dictionary, tfidf_dense, umap_neighbors = 15, dimensions = 2):\n",
    "    \"Transforms the TF-IDF matrix via UMAP and uses the result to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components = dimensions)\n",
    "    embed = reducer.fit_transform(tfidf_dense)\n",
    "    return get_phrase_vec_tfidf(doc_top_phrases, dictionary, embed)\n",
    "\n",
    "def get_doc_word_vectors_pretrained(doc_top_phrases, path_to_w2v_bin_file):\n",
    "    \"Uses pretrained Word2Vec embeddings to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    assert os.path.exists(path_to_w2v_bin_file), f'The file {path_to_w2v_bin_file} does not exist'\n",
    "    model = w2v_pretrained(path_to_w2v_bin_file)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)\n",
    "\n",
    "def get_doc_word_vectors_local(doc_top_phrases, raw_docs, my_docs):\n",
    "    \"Creates Word2Vec embeddings from the input corpus and uses them to create a list of lists corresponding to a word vectors for the top phrases of each sentence in each document\"\n",
    "    model = w2v_from_corpus(raw_docs, my_docs)\n",
    "    return get_phrase_vec_w2v(doc_top_phrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_sentences(doc_phrase_vecs, doc_top_phrases):\n",
    "    \"Filter the sentences, removing any that contain zero 'top phrases'\"\n",
    "    just_phrase_vecs = []\n",
    "    just_phrase_ids = []\n",
    "    just_phrase_text = []\n",
    "\n",
    "    # Getting all phrases which have \n",
    "    # Iterate documents\n",
    "    for doc_ix, phrase_vecs in enumerate(doc_phrase_vecs):\n",
    "        # Iterate sentences (each sentence has a phrase vector)\n",
    "        for phrase_ix, phrase_vec in enumerate(phrase_vecs):\n",
    "            phrase_id = f\"doc.{doc_ix}.sent.{phrase_ix}\"\n",
    "            this_text = doc_top_phrases[doc_ix][phrase_ix]\n",
    "\n",
    "            # Check phrase_vec is not zero or an array of zeros, these would imply the sentence has no \"top phrases\"\n",
    "            if isinstance(phrase_vec, numpy.ndarray) and any(numpy.zeros(len(phrase_vec)) != phrase_vec):\n",
    "                just_phrase_vecs.append(list(phrase_vec))\n",
    "                just_phrase_ids.append(phrase_id)\n",
    "                just_phrase_text.append(this_text[1])\n",
    "    return just_phrase_vecs, just_phrase_ids, just_phrase_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_optimal_k(just_phrase_vecs, save_chart = False, chart_file = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors for a range of k values and plots the silhoute coefficients \n",
    "    (this is a function of mean intra-cluster distance and mean nearest-cluster distance). \n",
    "    Returns the optimal k-value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_k = min(len(just_phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [(silhouette_score(just_phrase_vecs, KMeans(i).fit(just_phrase_vecs).predict(just_phrase_vecs))) for i in k_range]\n",
    "    fig = plt.plot(k_range, score)\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_k = k_range[numpy.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(k, just_phrase_vecs):\n",
    "    \"Use K-means algorithm to cluster phrase vectors\"\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(just_phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(just_phrase_vecs)\n",
    "    #TODO: this is never used before being overwritten later. Remove?\n",
    "    dist = pairwise_distances(just_phrase_vecs, metric='euclidean') \n",
    "    return cluster_assignments, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_linkage_matrix(just_phrase_vecs, dist_metric):\n",
    "    \"Creates a linkage matrix by calculating distance between phrase vectors\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(just_phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(just_phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(just_phrase_vecs, linkage_matrix, save_chart = False, chart_file = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    max_h = min(len(just_phrase_vecs), 100)\n",
    "    # QUESTION: What is the limiting factor for the min/max in this range?\n",
    "#     h_range = range(30, max_h) \n",
    "    h_range = range(1,10) \n",
    "    h_score = [(silhouette_score(just_phrase_vecs, [x[0] for x in cut_tree(linkage_matrix, height=i)] )) for i in h_range]\n",
    "\n",
    "    fig = plt.plot(h_range, h_score)\n",
    "\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    optimal_h = h_range[numpy.argmax(h_score)]\n",
    "    return optimal_h\n",
    "\n",
    "def get_cluster_assignments_hac(linkage_matrix, height):\n",
    "    \"Use Hierarchical Agglomerative Clustering to cluster phrase vectors\"\n",
    "    return [x[0] for x in cut_tree(linkage_matrix, height=height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def visualize_umap(just_phrase_ids, cluster_assignments, just_phrase_text, dist, umap_neighbors = 15, save_chart = False, chart_file = \"UMAP.html\"):\n",
    "    \"Visualize the clusters using UMAP\"\n",
    "    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42)\n",
    "    embedding = reducer.fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, UMAP_cluster=cluster_assignments, phrase=just_phrase_text, x=embedding[:, 0], y=embedding[:, 1]))\n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"UMAP_cluster\", hover_data=[\"phrase\", \"label\",\"UMAP_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_mds(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"MDS.html\"):\n",
    "    \"Visualize the clusters using Multi-Dimensional Scaling (MDS)\"\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "    pos = mds.fit_transform(dist)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, MDS_cluster=cluster_assignments, label=just_phrase_ids, phrase=just_phrase_text)) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"MDS_cluster\", hover_data=[\"phrase\", \"label\",\"MDS_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)\n",
    "        \n",
    "def visualize_svd(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = \"SVD.html\"):\n",
    "    \"Visualize the clusters using Singular Value Decomposition (SVD)\"\n",
    "    svd2d =  TruncatedSVD(n_components = 2, random_state = 42).fit_transform(dist)\n",
    "    df = pd.DataFrame(dict(label=just_phrase_ids, SVD_cluster=cluster_assignments, phrase=just_phrase_text, x=svd2d[:, 0], y=svd2d[:, 1])) \n",
    "    fig = px.scatter(df, x=\"x\", y=\"y\", hover_name=\"label\", color=\"SVD_cluster\", hover_data=[\"phrase\", \"label\",\"SVD_cluster\"], color_continuous_scale='rainbow')\n",
    "    fig.show()\n",
    "    if save_chart:\n",
    "        plotly.offline.plot(fig, filename=chart_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cluster_topics(cluster_assignments, just_phrase_ids, my_docs):\n",
    "    \"Get list of phrases by document/sentence\"\n",
    "    cluster_topics = []\n",
    "    for c in set(cluster_assignments):\n",
    "        #QUESTION: I removed the condition len(x.split(\".\")) > 1 is there a case when that wouldn't be True?\n",
    "        # Parse (doc_id, sent_id) \"coordinate\" pairs from dataframe label column\n",
    "        cluster_mask = numpy.asarray(cluster_assignments) == c\n",
    "        # TODO: Consolidate the next two lines if Amy's okay with storing just_phrase_ids as a list of tuples\n",
    "        topic_labels = numpy.asarray(just_phrase_ids)[cluster_mask]\n",
    "        sent_coords = [[int(x.split(\".\")[1]), int(x.split(\".\")[3])] for x in topic_labels] \n",
    "\n",
    "        ## sent_coords has each element formatted as [doc number, sentence number]\n",
    "        cluster_terms = []\n",
    "        for doc_id, sent_id in sent_coords:\n",
    "            sent = my_docs[doc_id][sent_id]\n",
    "            # This isn't used. Can we remove?\n",
    "#             pos = my_docs_pos[doc_id][sent_id] \n",
    "            cluster_terms.append(sent)\n",
    "\n",
    "        cluster_topics.append(cluster_terms)\n",
    "    return cluster_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted helpers.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted index.ipynb.\n",
      "Converted nlp_helpers.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
