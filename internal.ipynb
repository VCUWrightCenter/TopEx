{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nbdev.imports import *\n",
    "from nbdev.export import *\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import ward, cut_tree, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "\n",
    "> Generic helper methods for loading data, etc. Not intended to be imported by the end user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#DELETE\n",
    "def get_cluster_sent_coords(cluster_id, just_phrase_ids, cluster_assignments):\n",
    "    \"Parse (doc_id, sent_id) 'coordinate' pairs\"\n",
    "    #QUESTION: I removed the condition len(x.split(\".\")) > 1 is there a case when that wouldn't be True?\n",
    "    cluster_mask = np.asarray(cluster_assignments) == cluster_id\n",
    "    # TODO: Consolidate the next two lines if Amy's okay with storing just_phrase_ids as a list of tuples\n",
    "    topic_labels = np.asarray(just_phrase_ids)[cluster_mask]\n",
    "    sent_coords = [[int(x.split(\".\")[1]), int(x.split(\".\")[3])] for x in topic_labels] \n",
    "    return sent_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract phrases from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_phrase_column(sent, window_size, feature_names, include_input_in_tfidf, tdm, token_averages):\n",
    "    adj_adv_pos_list = [\"JJ\",\"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]\n",
    "    phrase_scores = []\n",
    "    top_phrase = None\n",
    "    top_score = -1\n",
    "    \n",
    "    # Iterate phrases (sub-sentences of length window_size)\n",
    "    for p in range(len(sent.tokens) - window_size + 1):\n",
    "        window = slice(p, p + window_size)\n",
    "        phrase = sent.tokens[window]\n",
    "        phrase_pos = sent.pos_tags[window]\n",
    "\n",
    "        weight = 1 + abs(TextBlob(\" \".join(phrase)).sentiment.polarity)\n",
    "        score = 0\n",
    "\n",
    "        for i, token in enumerate(phrase):\n",
    "            # Skip tokens not in feature_names\n",
    "            if token not in list(feature_names.keys()):\n",
    "                continue\n",
    "\n",
    "            pos = phrase_pos[i][1]\n",
    "            token_ix = feature_names[token]\n",
    "\n",
    "            # Token score comes from TF-IDf matrix if include_input_in_tfidf is set, otherwise, use tokens averages\n",
    "            token_score = tdm[token_ix, sent.doc_id] if include_input_in_tfidf else token_averages[token_ix];\n",
    "\n",
    "            # Scale token_score by 3x if the token is an adjective or adverb\n",
    "            score += (token_score * 3) if pos in adj_adv_pos_list else token_score\n",
    "        \n",
    "        # Update top_score if necessary\n",
    "        phrase_score = score * weight\n",
    "        if phrase_score > top_score:\n",
    "            top_phrase = phrase\n",
    "            top_score = phrase_score\n",
    "\n",
    "    sent.phrase = top_phrase\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_vec_column_tfidf(sent, dictionary, term_matrix):\n",
    "    \"Lambda function for setting the vec column to be applied to each row in a dataframe\"\n",
    "    if sent.phrase is not None:\n",
    "        vec_ids = [x[0] for x in dictionary.doc2bow(sent.phrase)]\n",
    "        sent.vec = term_matrix[vec_ids].sum(axis=0)\n",
    "    return sent\n",
    "\n",
    "def set_vec_column_w2v(sent, model):\n",
    "    \"Lambda function for setting the vec column to be applied to each row in a dataframe\"\n",
    "    if sent.phrase is not None:\n",
    "        tokens = [token for token in sent.phrase if token in model.wv.vocab]\n",
    "        sent.vec = model[tokens].sum(axis=0)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### Hierarchical Agglomerative Clustering (HAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_hac(phrase_vecs, linkage_matrix, height):\n",
    "    \"Assigns clusters to a list of word vectors for a given `height` and calculates the silhouette score of the clustering.\"\n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "\n",
    "def get_tree_height(root):\n",
    "    \"Gets the height of a binary tree.\"\n",
    "    if root is None:\n",
    "        return 1\n",
    "    return max(get_tree_height(root.left), get_tree_height(root.right)) + 1\n",
    "\n",
    "def get_linkage_matrix(phrase_vecs, dist_metric):\n",
    "    \"Creates a linkage matrix by calculating distance between phrase vectors.\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(data, linkage_matrix, show_dendrogram = False, show_chart = True, save_chart = False, chart_file = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    max_h = get_tree_height(hierarchy.to_tree(linkage_matrix)) + 1\n",
    "    h_range = range(2,max_h)\n",
    "    h_scores = [get_silhouette_score_hac(phrase_vecs, linkage_matrix, h) for h in h_range]\n",
    "    \n",
    "    # Optionally display the clustering dendrogram\n",
    "    if show_dendrogram:\n",
    "        dendrogram(linkage_matrix)\n",
    "        plt.show()\n",
    "        \n",
    "    # Optionally display the graph of silhouette score by height\n",
    "    if show_chart:\n",
    "        fig = plt.plot(h_range, h_scores)\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally save the graph of silhouette score by height to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_h is height value with the highest silhouette score\n",
    "    optimal_height = h_range[np.argmax(h_scores)]\n",
    "    return optimal_height\n",
    "\n",
    "def get_cluster_assignments_hac(data, dist_metric, height = None, show_dendrogram = False, show_chart = False):\n",
    "    \"Use Hierarchical Agglomerative Clustering (HAC) to cluster phrase vectors\"\n",
    "    linkage_matrix = get_linkage_matrix(list(data.vec), dist_metric)\n",
    "    \n",
    "    # Use optimal height if no height is specified\n",
    "    if height is None:\n",
    "        height = get_optimal_height(data, linkage_matrix, show_dendrogram, show_chart)\n",
    "    \n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_kmeans(phrase_vecs, k):\n",
    "    \"Assigns clusters to a list of word vectors for a given `k` and calculates the silhouette score of the clustering.\"\n",
    "    cluster_assignments = KMeans(k).fit(phrase_vecs).predict(phrase_vecs)\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "    \n",
    "def get_optimal_k(data, show_chart = True, save_chart = False, chart_file = \"KmeansSilhouette.png\"):\n",
    "    \"Calculates the optimal k-value (highest silhoute coefficient). Optionally prints a chart of silhouette score by k-value or saves it to disk.\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    max_k = min(len(phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [get_silhouette_score_kmeans(phrase_vecs, i) for i in k_range]\n",
    "    \n",
    "    # Optionally display the graph of silhouette score by k-value\n",
    "    if show_chart:\n",
    "        fig = plt.plot(k_range, score)\n",
    "        \n",
    "    # Optionally save the graph of silhouette score by k-value to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_k is k value with the highest silhouette score\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(data, k = None, show_chart = False):\n",
    "    \"Use K-means algorithm to cluster phrase vectors\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    \n",
    "    # Use optimal k if no k-value is specified\n",
    "    if k is None:\n",
    "        k = get_optimal_k(data, show_chart)\n",
    "    \n",
    "    # Assign clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(phrase_vecs)\n",
    "    \n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted helpers.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted index.ipynb.\n",
      "Converted nlp_helpers.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
