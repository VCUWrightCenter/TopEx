{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "import medtop.preprocessing as preprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import ward, cut_tree, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal\n",
    "\n",
    "> Internal MedTop methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are intended for internal use only. MedTop users should only need to import the core module in order to fully use every feature of the MedTop library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_data(raw_docs:DataFrame, save_results:bool, file_name:str, stop_words_file:str):\n",
    "    \"\"\"\n",
    "    Imports and pre-processes the documents from the `raw_docs` dataframe\n",
    "    \n",
    "    Document pre-processing is handled in [`tokenize_and_stem`](/medtop/preprocessing#tokenize_and_stem).\n",
    "    `path_to_file_list` is a path to a text file containing a list of files to be processed separated by line breaks.\n",
    "    \n",
    "    Returns (DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dataframe containing the doc_id, file name, and raw text of each document\n",
    "    doc_cols = [\"doc_id\", \"doc_name\", \"text\", \"tokens\"]\n",
    "    doc_rows = []\n",
    "    \n",
    "    # Create a dataframe containing the id, doc_id, sent_id, raw text, tokens, and PoS tags for each sentence\n",
    "    data_cols = [\"id\", \"doc_id\", \"sent_id\", \"text\", \"tokens\", \"pos_tags\"]    \n",
    "    data_rows = []\n",
    "    \n",
    "    for doc_id in range(len(raw_docs)):\n",
    "        doc = raw_docs.iloc[doc_id]\n",
    "        \n",
    "        # Pre-process document into cleaned sentences\n",
    "        sent_tokens, sent_pos, raw_sent = preprocessing.tokenize_and_stem(doc.text, stop_words_file=stop_words_file)\n",
    "\n",
    "        # Populate a row in data for each sentence in the document\n",
    "        for sent_id, _ in enumerate(sent_tokens):\n",
    "            row_id = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            sent_text = raw_sent[sent_id]\n",
    "            tokens = sent_tokens[sent_id]\n",
    "            pos_tags = sent_pos[sent_id]\n",
    "            data_rows.append(Series([row_id, doc_id, sent_id, sent_text, tokens, pos_tags], index=data_cols))\n",
    "        \n",
    "        # Populate a row in doc_df for each file loaded\n",
    "        doc_tokens = [token for sent in sent_tokens for token in sent]\n",
    "        doc_row = Series([doc_id, doc.doc_name, doc.text, doc_tokens], index=doc_cols)\n",
    "        doc_rows.append(doc_row)\n",
    "    \n",
    "    # Create dataframes from lists of Series\n",
    "    data = DataFrame(data_rows)\n",
    "    doc_df = DataFrame(doc_rows)\n",
    "    \n",
    "    # Optionally save the results to disk\n",
    "    if save_results:\n",
    "        sentences_to_disk(data, file_name)\n",
    "\n",
    "    return data, doc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_phrase(sent:Series, window_size:int, feature_names:dict, include_input_in_tfidf:bool, tdm:np.ndarray, \n",
    "               token_averages:np.ndarray):\n",
    "    \"\"\"\n",
    "    Finds the most expressive phrase in a sentence. This function is called in a lambda expression in `core.get_phrases`.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    adj_adv_pos_list = [\"JJ\",\"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]\n",
    "    phrase_scores = []\n",
    "    top_phrase = None\n",
    "    top_score = -1\n",
    "    \n",
    "    # Iterate phrases (sub-sentences of length window_size)\n",
    "    for p in range(len(sent.tokens) - window_size + 1):\n",
    "        window = slice(p, p + window_size)\n",
    "        phrase = sent.tokens[window]\n",
    "        phrase_pos = sent.pos_tags[window]\n",
    "\n",
    "        weight = 1 + abs(TextBlob(\" \".join(phrase)).sentiment.polarity)\n",
    "        score = 0\n",
    "\n",
    "        for i, token in enumerate(phrase):\n",
    "            # Skip tokens not in feature_names\n",
    "            if token not in list(feature_names.keys()):\n",
    "                continue\n",
    "\n",
    "            pos = phrase_pos[i][1]\n",
    "            token_ix = feature_names[token]\n",
    "\n",
    "            # Token score comes from TF-IDf matrix if include_input_in_tfidf is set, otherwise, use tokens averages\n",
    "            token_score = tdm[token_ix, sent.doc_id] if include_input_in_tfidf else token_averages[token_ix];\n",
    "\n",
    "            # Scale token_score by 3x if the token is an adjective or adverb\n",
    "            score += (token_score * 3) if pos in adj_adv_pos_list else token_score\n",
    "        \n",
    "        # Update top_score if necessary\n",
    "        phrase_score = score * weight\n",
    "        if phrase_score > top_score:\n",
    "            top_phrase = phrase\n",
    "            top_score = phrase_score\n",
    "\n",
    "    return top_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_vector_tfidf(sent:Series, dictionary:gensim.corpora.dictionary.Dictionary, term_matrix:np.ndarray):\n",
    "    \"\"\"\n",
    "    Create a word vector for a given sentence using a term matrix. \n",
    "    This function is called in a lambda expression in `core.get_vectors`.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    vec_ids = [x[0] for x in dictionary.doc2bow(sent.phrase)]\n",
    "    return term_matrix[vec_ids].sum(axis=0)\n",
    "\n",
    "def get_vector_w2v(sent:Series, model:gensim.models.keyedvectors.Word2VecKeyedVectors):\n",
    "    \"\"\"\n",
    "    Create a word vector for a given sentence using a Word2Vec model.\n",
    "    This function is called in a lambda expression in `core.get_vectors`.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    tokens = [token for token in sent.phrase if token in model.wv.vocab]\n",
    "    return model[tokens].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def w2v_pretrained(bin_file:str):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Word2Vec model from a bin file.\n",
    "\n",
    "    Returns gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \"\"\"\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(bin_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_hac(phrase_vecs:list, linkage_matrix:np.ndarray, height:int):\n",
    "    \"\"\"\n",
    "    Assigns clusters to a list of word vectors for a given `height` and calculates the silhouette score of the clustering.\n",
    "\n",
    "    Returns float\n",
    "    \"\"\"\n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "\n",
    "def get_tree_height(root:hierarchy.ClusterNode):\n",
    "    \"\"\"\n",
    "    Gets the height of a binary tree.\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    if root is None:\n",
    "        return 1\n",
    "    return max(get_tree_height(root.left), get_tree_height(root.right)) + 1\n",
    "\n",
    "def get_linkage_matrix(phrase_vecs:list, dist_metric:str):\n",
    "    \"\"\"\n",
    "    Creates a linkage matrix by calculating distance between phrase vectors.\n",
    "\n",
    "    Returns np.ndarray\n",
    "    \"\"\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(data:DataFrame, linkage_matrix:np.ndarray, show_dendrogram:bool = False, show_chart:bool = True, \n",
    "                       save_chart:bool = False, chart_file:str = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    # Maximum cut point height is the height of the tree\n",
    "    max_h = get_tree_height(hierarchy.to_tree(linkage_matrix)) + 1\n",
    "    h_range = range(2,max_h)\n",
    "    phrase_vecs = list(data.vec)\n",
    "    h_scores = [get_silhouette_score_hac(phrase_vecs, linkage_matrix, h) for h in h_range]\n",
    "    \n",
    "    # Optionally display the clustering dendrogram\n",
    "    if show_dendrogram:\n",
    "        dendrogram(linkage_matrix)\n",
    "        plt.show()\n",
    "        \n",
    "    # Optionally display the graph of silhouette score by height\n",
    "    if show_chart:\n",
    "        fig = plt.plot(h_range, h_scores)\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally save the graph of silhouette score by height to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_h is height value with the highest silhouette score\n",
    "    optimal_height = h_range[np.argmax(h_scores)]\n",
    "    return optimal_height\n",
    "\n",
    "def get_clusters_hac(data:DataFrame, dist_metric:str, height:int = None, show_dendrogram:bool = False, \n",
    "                     show_chart:bool = False):\n",
    "    \"\"\"\n",
    "    Use Hierarchical Agglomerative Clustering (HAC) to cluster phrase vectors\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    linkage_matrix = get_linkage_matrix(list(data.vec), dist_metric)\n",
    "    \n",
    "    # Use optimal height if no height is specified\n",
    "    if height is None:\n",
    "        height = get_optimal_height(data, linkage_matrix, show_dendrogram, show_chart)\n",
    "    \n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_kmeans(phrase_vecs:list, k:int):\n",
    "    \"\"\"\n",
    "    Assigns clusters to a list of word vectors for a given `k` and calculates the silhouette score of the clustering.\n",
    "\n",
    "    Returns float\n",
    "    \"\"\"\n",
    "    cluster_assignments = KMeans(k).fit(phrase_vecs).predict(phrase_vecs)\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "    \n",
    "def get_optimal_k(data:DataFrame, show_chart:bool = True, save_chart:bool = False, \n",
    "                  chart_file:str = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Calculates the optimal k-value (highest silhoute coefficient). \n",
    "    Optionally prints a chart of silhouette score by k-value or saves it to disk.\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    max_k = min(len(phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [get_silhouette_score_kmeans(phrase_vecs, i) for i in k_range]\n",
    "    \n",
    "    # Optionally display the graph of silhouette score by k-value\n",
    "    if show_chart:\n",
    "        fig = plt.plot(k_range, score)\n",
    "        \n",
    "    # Optionally save the graph of silhouette score by k-value to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_k is k value with the highest silhouette score\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_clusters_kmeans(data:DataFrame, k:int = None, show_chart:bool = False):\n",
    "    \"\"\"\n",
    "    Use K-means algorithm to cluster phrase vectors\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    \n",
    "    # Use optimal k if no k-value is specified\n",
    "    if k is None:\n",
    "        k = get_optimal_k(data, show_chart)\n",
    "    \n",
    "    # Assign clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(phrase_vecs)\n",
    "    \n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_topics_from_docs(docs:list, topic_count:int):\n",
    "    \"\"\"\n",
    "    Gets a list of `topic_count` topics for each list of tokens in `docs`.\n",
    "    \n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "\n",
    "    # Use Latent Dirichlet Allocation (LDA) to find the main topics \n",
    "    lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "    topics_matrix = lda.show_topics(formatted=False, num_words=topic_count)\n",
    "    topics = list(np.array(topics_matrix[0][1])[:,0])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "def df_to_disk(df:DataFrame, file_name:str, mode:str=\"w\", header:bool=True, sep='\\t'):\n",
    "    \"\"\"\n",
    "    Writes a dataframe to disk as a tab delimited file.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(file_name, sep=sep, mode=mode, header=header, encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "    if mode == \"w\":\n",
    "        print(f\"Results saved to {file_name}\")\n",
    "    \n",
    "def sentences_to_disk(data:DataFrame, file_name:str = 'output/DocumentSentenceList.txt'):\n",
    "    \"\"\"\n",
    "    Writes the raw sentences to a file organized by document and sentence number.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    df = data[[\"id\", \"text\"]].copy()\n",
    "    df_to_disk(df, file_name)\n",
    "    \n",
    "    \n",
    "def write_cluster(cluster_rows:DataFrame, file_name:str, mode:str = 'a', header:bool=False):\n",
    "    \"\"\"\n",
    "    Appends the rows for a single cluster to disk.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    df_to_disk(cluster_rows, file_name, mode=mode, header=header)\n",
    "    \n",
    "    \n",
    "def clusters_to_disk(data:DataFrame, doc_df:DataFrame, cluster_df:DataFrame, \n",
    "                     file_name:str = 'output/TopicClusterResults.txt'):\n",
    "    \"\"\"\n",
    "    Writes the sentences and phrases to a file organized by cluster and document.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    # Create a dataframe containing the data to be saved to disk\n",
    "    df = data[[\"cluster\", \"doc_id\", \"sent_id\", \"text\", \"phrase\"]].copy()\n",
    "    doc_names = [doc_df.loc[c].doc_name for c in data.doc_id]\n",
    "    df.insert(loc=3, column='doc_name', value=doc_names)\n",
    "    df.sort_values(by=[\"cluster\", \"doc_id\", \"sent_id\"], inplace=True)\n",
    "\n",
    "    # Write document header\n",
    "    cluster_rows = pd.DataFrame(None, columns=df.columns)\n",
    "    write_cluster(cluster_rows, file_name, mode = 'w', header=True)\n",
    "\n",
    "    # Write each cluster\n",
    "    for c in set(data.cluster):\n",
    "        # Write a cluster header containing the main topics for each cluster\n",
    "        with open(file_name, encoding=\"utf-8\", mode = 'a') as file:\n",
    "            keywords = ', '.join(cluster_df.loc[c, 'topics'])\n",
    "            file.write(f\"Cluster: {c}; Keywords: [{keywords}]\\n\")\n",
    "\n",
    "        # Write the sentences in each cluster\n",
    "        cluster_rows = df[df.cluster == c].copy()\n",
    "        write_cluster(cluster_rows, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
