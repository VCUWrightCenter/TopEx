{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from nbdev.imports import *\n",
    "from nbdev.export import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import ward, cut_tree, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal\n",
    "\n",
    "> Internal MedTop methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_phrase(sent, window_size, feature_names, include_input_in_tfidf, tdm, token_averages):\n",
    "    \"Finds the most expressive phrase in a sentence.\"\n",
    "    adj_adv_pos_list = [\"JJ\",\"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"]\n",
    "    phrase_scores = []\n",
    "    top_phrase = None\n",
    "    top_score = -1\n",
    "    \n",
    "    # Iterate phrases (sub-sentences of length window_size)\n",
    "    for p in range(len(sent.tokens) - window_size + 1):\n",
    "        window = slice(p, p + window_size)\n",
    "        phrase = sent.tokens[window]\n",
    "        phrase_pos = sent.pos_tags[window]\n",
    "\n",
    "        weight = 1 + abs(TextBlob(\" \".join(phrase)).sentiment.polarity)\n",
    "        score = 0\n",
    "\n",
    "        for i, token in enumerate(phrase):\n",
    "            # Skip tokens not in feature_names\n",
    "            if token not in list(feature_names.keys()):\n",
    "                continue\n",
    "\n",
    "            pos = phrase_pos[i][1]\n",
    "            token_ix = feature_names[token]\n",
    "\n",
    "            # Token score comes from TF-IDf matrix if include_input_in_tfidf is set, otherwise, use tokens averages\n",
    "            token_score = tdm[token_ix, sent.doc_id] if include_input_in_tfidf else token_averages[token_ix];\n",
    "\n",
    "            # Scale token_score by 3x if the token is an adjective or adverb\n",
    "            score += (token_score * 3) if pos in adj_adv_pos_list else token_score\n",
    "        \n",
    "        # Update top_score if necessary\n",
    "        phrase_score = score * weight\n",
    "        if phrase_score > top_score:\n",
    "            top_phrase = phrase\n",
    "            top_score = phrase_score\n",
    "\n",
    "    return top_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_vector_tfidf(sent, dictionary, term_matrix):\n",
    "    \"Create a word vector for a given sentence using a term matrix.\"\n",
    "    vec_ids = [x[0] for x in dictionary.doc2bow(sent.phrase)]\n",
    "    return term_matrix[vec_ids].sum(axis=0)\n",
    "\n",
    "def get_vector_w2v(sent, model):\n",
    "    \"Create a word vector for a given sentence using a Word2Vec model.\"\n",
    "    tokens = [token for token in sent.phrase if token in model.wv.vocab]\n",
    "    return model[tokens].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def w2v_pretrained(bin_file):\n",
    "    \"Load a pre-trained Word2Vec model from a bin file.\"\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(bin_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_hac(phrase_vecs, linkage_matrix, height):\n",
    "    \"Assigns clusters to a list of word vectors for a given `height` and calculates the silhouette score of the clustering.\"\n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "\n",
    "def get_tree_height(root):\n",
    "    \"Gets the height of a binary tree.\"\n",
    "    if root is None:\n",
    "        return 1\n",
    "    return max(get_tree_height(root.left), get_tree_height(root.right)) + 1\n",
    "\n",
    "def get_linkage_matrix(phrase_vecs, dist_metric):\n",
    "    \"Creates a linkage matrix by calculating distance between phrase vectors.\"\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(phrase_vecs)\n",
    "    else:\n",
    "        dist = pairwise_distances(phrase_vecs, metric=dist_metric)\n",
    "    return ward(dist)\n",
    "\n",
    "def get_optimal_height(data, linkage_matrix, show_dendrogram = False, show_chart = True, save_chart = False, chart_file = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "    \"\"\"\n",
    "    # Maximum cut point height is the height of the tree\n",
    "    max_h = get_tree_height(hierarchy.to_tree(linkage_matrix)) + 1\n",
    "    h_range = range(2,max_h)\n",
    "    phrase_vecs = list(data.vec)\n",
    "    h_scores = [get_silhouette_score_hac(phrase_vecs, linkage_matrix, h) for h in h_range]\n",
    "    \n",
    "    # Optionally display the clustering dendrogram\n",
    "    if show_dendrogram:\n",
    "        dendrogram(linkage_matrix)\n",
    "        plt.show()\n",
    "        \n",
    "    # Optionally display the graph of silhouette score by height\n",
    "    if show_chart:\n",
    "        fig = plt.plot(h_range, h_scores)\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally save the graph of silhouette score by height to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_h is height value with the highest silhouette score\n",
    "    optimal_height = h_range[np.argmax(h_scores)]\n",
    "    return optimal_height\n",
    "\n",
    "def get_cluster_assignments_hac(data, dist_metric, height = None, show_dendrogram = False, show_chart = False):\n",
    "    \"Use Hierarchical Agglomerative Clustering (HAC) to cluster phrase vectors\"\n",
    "    linkage_matrix = get_linkage_matrix(list(data.vec), dist_metric)\n",
    "    \n",
    "    # Use optimal height if no height is specified\n",
    "    if height is None:\n",
    "        height = get_optimal_height(data, linkage_matrix, show_dendrogram, show_chart)\n",
    "    \n",
    "    cluster_assignments = [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_kmeans(phrase_vecs, k):\n",
    "    \"Assigns clusters to a list of word vectors for a given `k` and calculates the silhouette score of the clustering.\"\n",
    "    cluster_assignments = KMeans(k).fit(phrase_vecs).predict(phrase_vecs)\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "    \n",
    "def get_optimal_k(data, show_chart = True, save_chart = False, chart_file = \"KmeansSilhouette.png\"):\n",
    "    \"Calculates the optimal k-value (highest silhoute coefficient). Optionally prints a chart of silhouette score by k-value or saves it to disk.\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    max_k = min(len(phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [get_silhouette_score_kmeans(phrase_vecs, i) for i in k_range]\n",
    "    \n",
    "    # Optionally display the graph of silhouette score by k-value\n",
    "    if show_chart:\n",
    "        fig = plt.plot(k_range, score)\n",
    "        \n",
    "    # Optionally save the graph of silhouette score by k-value to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_k is k value with the highest silhouette score\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(data, k = None, show_chart = False):\n",
    "    \"Use K-means algorithm to cluster phrase vectors\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    \n",
    "    # Use optimal k if no k-value is specified\n",
    "    if k is None:\n",
    "        k = get_optimal_k(data, show_chart)\n",
    "    \n",
    "    # Assign clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(phrase_vecs)\n",
    "    \n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "def df_to_disk(df, file_name, mode=\"w\", header=True):\n",
    "    \"Writes a dataframe to disk as a tab delimited file.\"\n",
    "    \n",
    "    df.to_csv(file_name, sep='\\t', mode=mode, header=header, encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "    if mode == \"w\":\n",
    "        print(f\"Results saved to {file_name}\")\n",
    "    \n",
    "    \n",
    "def sentences_to_disk(data, file_name = 'output/DocumentSentenceList.txt'):\n",
    "    \"Writes the raw sentences to a file organized by document and sentence number.\"\n",
    "    \n",
    "    df = data[[\"id\", \"text\"]].copy()\n",
    "    df_to_disk(df, file_name)\n",
    "    \n",
    "    \n",
    "def write_cluster(cluster_rows, file_name, mode = 'a', header=False):\n",
    "    \"Appends the rows for a single cluster to disk.\"\n",
    "    \n",
    "    df_to_disk(cluster_rows, file_name, mode=mode, header=header)\n",
    "    \n",
    "    \n",
    "def clusters_to_disk(data, doc_df, cluster_df, file_name = 'output/TopicClusterResults.txt'):\n",
    "    \"Writes the sentences and phrases to a file organized by cluster and document.\"\n",
    "\n",
    "    # Create a dataframe containing the data to be saved to disk\n",
    "    df = data[[\"cluster\", \"doc_id\", \"sent_id\", \"phrase\", \"text\"]].copy()\n",
    "    file_names = [doc_df.loc[c].file for c in data.doc_id]\n",
    "    df.insert(loc=2, column='file', value=file_names)\n",
    "    df.sort_values(by=[\"cluster\", \"doc_id\", \"sent_id\"], inplace=True)\n",
    "    \n",
    "    # Write document header\n",
    "    cluster_rows = pd.DataFrame(None, columns=data.columns)\n",
    "    write_cluster(cluster_rows, file_name, mode = 'w', header=True)\n",
    "    \n",
    "    # Write each cluster\n",
    "    for c in set(data.cluster):\n",
    "        # Write a cluster header containing the main topics for each cluster\n",
    "        with open(file_name, encoding=\"utf-8\", mode = 'a') as file:\n",
    "            keywords = ', '.join(cluster_df.loc[c, 'topics'])\n",
    "            file.write(f\"Cluster: {c}; Keywords: [{keywords}]\\n\")\n",
    "            \n",
    "        # Write the sentences in each cluster\n",
    "        cluster_rows = df[df.cluster == c].copy()\n",
    "        write_cluster(cluster_rows, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Sandbox.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
