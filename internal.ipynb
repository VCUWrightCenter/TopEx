{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import csv\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import ward, cut_tree, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal\n",
    "\n",
    "> Internal TopEx methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are intended for internal use only. TopEx users should only need to import the core module in order to fully use every feature of the TopEx library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def score_phrase(phrase:list, score:float, include_sentiment:bool):\n",
    "    \"Gets the numerical polarity of a list of tokens\"\n",
    "    weight = 1\n",
    "    \n",
    "    # Optionally weight the polarity of the phrase\n",
    "    if include_sentiment:\n",
    "        weight += abs(TextBlob(\" \".join(phrase)).sentiment.polarity)\n",
    "            \n",
    "    return score * weight\n",
    "\n",
    "def score_token(token:str, pos:str, doc_id:int, vocab:dict, tfidf:np.ndarray, max_token_scores:np.ndarray,\n",
    "                tfidf_corpus:str, include_sentiment:bool):\n",
    "    \"Calculates the importance of a single token\"\n",
    "    score = 0\n",
    "    weight = 1\n",
    "    \n",
    "    # Only score tokens in vocabulary\n",
    "    if token in vocab:\n",
    "        # Token score comes from TF-IDf matrix if tfidf_corpus='clustering' is set, otherwise, use max token score\n",
    "        token_ix = vocab[token]\n",
    "        score = tfidf[token_ix, doc_id] if tfidf_corpus=='clustering' else max_token_scores[token_ix];\n",
    "\n",
    "        # Scale token_score by 3x if including sentiment and the token is an adjective or adverb\n",
    "        if include_sentiment and pos in ['ADJ', 'ADV']:\n",
    "            score *= 3\n",
    "\n",
    "    return score\n",
    "    \n",
    "def get_phrase(sent:Series, window_size:int, vocab:dict, tfidf_corpus:str, tfidf:np.ndarray, \n",
    "                                                       max_token_scores:np.ndarray, include_sentiment:bool):\n",
    "    \"\"\"\n",
    "    Finds the most expressive phrase in a sentence. This function is called in a lambda expression in `core.get_phrases`. \n",
    "    Passing `include_sentiment=False` will weight all tokens equally, ignoring sentiment and part of speech.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    tokens = sent.tokens\n",
    "    \n",
    "    # Score each token in the sentence\n",
    "    token_scores = [\n",
    "        score_token(t, sent.pos_tags[i], sent.doc_id, vocab, tfidf, max_token_scores, tfidf_corpus, include_sentiment)\n",
    "        for i, t in enumerate(tokens)]\n",
    "    \n",
    "    # Score each phrase in the sentence\n",
    "    window_cnt = len(tokens) - window_size + 1\n",
    "    windows = [slice(w, w + window_size) for w in range(window_cnt)]\n",
    "    phrase_scores = [score_phrase(tokens[w], sum(token_scores[w]), include_sentiment) for w in windows]\n",
    "\n",
    "    # Find the phrase with the highest score\n",
    "    phrase = sent.tokens[windows[np.argmax(phrase_scores)]]\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_vector_tfidf(sent:Series, dictionary:gensim.corpora.dictionary.Dictionary, term_matrix:np.ndarray):\n",
    "    \"\"\"\n",
    "    Create a word vector for a given sentence using a term matrix. \n",
    "    This function is called in a lambda expression in `core.get_vectors`.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    vec_ids = [x[0] for x in dictionary.doc2bow(sent.phrase)]\n",
    "    return term_matrix[vec_ids].sum(axis=0)\n",
    "\n",
    "def get_vector_w2v(sent:Series, model:gensim.models.keyedvectors.Word2VecKeyedVectors):\n",
    "    \"\"\"\n",
    "    Create a word vector for a given sentence using a Word2Vec model.\n",
    "    This function is called in a lambda expression in `core.get_vectors`.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    tokens = [token for token in sent.phrase if token in model.wv.vocab]\n",
    "    return model[tokens].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def w2v_pretrained(bin_file:str):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Word2Vec model from a bin file.\n",
    "\n",
    "    Returns gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \"\"\"\n",
    "    return gensim.models.KeyedVectors.load_word2vec_format(bin_file, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cluster_assignments_hac(linkage_matrix:np.ndarray, height:int):\n",
    "    \"\"\"\n",
    "    Assigns clusters by cutting the HAC dendrogram at the specified height.\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    return [x[0] for x in cut_tree(linkage_matrix, height=height)]\n",
    "\n",
    "def get_silhouette_score_hac(phrase_vecs:list, linkage_matrix:np.ndarray, height:int):\n",
    "    \"\"\"\n",
    "    Assigns clusters to a list of word vectors for a given `height` and calculates the silhouette score of the clustering.\n",
    "\n",
    "    Returns float\n",
    "    \"\"\"\n",
    "    cluster_assignments = get_cluster_assignments_hac(linkage_matrix, height)\n",
    "    \n",
    "    # Getting weird errors for trying invalid ranges for unknown reasons. Wrapping in try/except to handle\n",
    "    try:\n",
    "        score = silhouette_score(phrase_vecs, cluster_assignments)\n",
    "    except:\n",
    "        score = -1\n",
    "        \n",
    "    return score\n",
    "\n",
    "def get_tree_height(root:hierarchy.ClusterNode):\n",
    "    \"\"\"\n",
    "    Recursively finds the height of a binary tree.\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    if root is None:\n",
    "        return 1\n",
    "    return max(get_tree_height(root.left), get_tree_height(root.right)) + 1\n",
    "\n",
    "def get_optimal_height(data:DataFrame, linkage_matrix:np.ndarray, max_h:int, show_chart:bool = True, \n",
    "                       save_chart:bool = False, chart_file:str = \"HACSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Clusters the top phrase vectors and plots the silhoute coefficients for a range of dendrograph heights. \n",
    "    Returns the optimal height value (highest silhoute coefficient)\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    h_range = range(2,max_h)\n",
    "    phrase_vecs = list(data.vec)\n",
    "    h_scores = [get_silhouette_score_hac(phrase_vecs, linkage_matrix, h) for h in h_range]\n",
    "        \n",
    "    # Optionally display the graph of silhouette score by height\n",
    "    if show_chart:\n",
    "        fig = plt.plot(h_range, h_scores)\n",
    "        plt.show()\n",
    "\n",
    "    # Optionally save the graph of silhouette score by height to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_h is height value with the highest silhouette score\n",
    "    optimal_height = h_range[np.argmax(h_scores)]\n",
    "    return optimal_height\n",
    "    \n",
    "def get_clusters_hac(data:DataFrame, dist_metric:str, height:int = None, show_dendrogram:bool = False, \n",
    "                     show_chart:bool = False):\n",
    "    \"\"\"\n",
    "    Use Hierarchical Agglomerative Clustering (HAC) to cluster phrase vectors\n",
    "\n",
    "    Returns (list, np.ndarray, int)\n",
    "    \"\"\"   \n",
    "    # Create a linkage matrix\n",
    "    if dist_metric == \"cosine\":\n",
    "        dist = 1 - cosine_similarity(list(data.vec))\n",
    "    else:\n",
    "        dist = pairwise_distances(list(data.vec), metric=dist_metric)\n",
    "    linkage_matrix = ward(dist)\n",
    "    \n",
    "    # Maximum cut point height is the height of the tree\n",
    "    max_h = get_tree_height(hierarchy.to_tree(linkage_matrix)) + 1\n",
    "    \n",
    "    # Use optimal height if no height is specified\n",
    "    if height is None:\n",
    "        height = get_optimal_height(data, linkage_matrix, max_h, show_chart)\n",
    "    \n",
    "    cluster_assignments = get_cluster_assignments_hac(linkage_matrix, height)\n",
    "    \n",
    "    # Optionally display the clustering dendrogram\n",
    "    if show_dendrogram:\n",
    "        dendrogram(linkage_matrix)\n",
    "        plt.show()\n",
    "    \n",
    "    return cluster_assignments, linkage_matrix, max_h, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_silhouette_score_kmeans(phrase_vecs:list, k:int):\n",
    "    \"\"\"\n",
    "    Assigns clusters to a list of word vectors for a given `k` and calculates the silhouette score of the clustering.\n",
    "\n",
    "    Returns float\n",
    "    \"\"\"\n",
    "    cluster_assignments = KMeans(k).fit(phrase_vecs).predict(phrase_vecs)\n",
    "    return silhouette_score(phrase_vecs, cluster_assignments)\n",
    "    \n",
    "def get_optimal_k(data:DataFrame, show_chart:bool = True, save_chart:bool = False, \n",
    "                  chart_file:str = \"KmeansSilhouette.png\"):\n",
    "    \"\"\"\n",
    "    Calculates the optimal k-value (highest silhoute coefficient). \n",
    "    Optionally prints a chart of silhouette score by k-value or saves it to disk.\n",
    "\n",
    "    Returns int\n",
    "    \"\"\"\n",
    "    phrase_vecs = list(data.vec)\n",
    "    max_k = min(len(phrase_vecs), 100)\n",
    "    k_range = range(2, max_k)\n",
    "    score = [get_silhouette_score_kmeans(phrase_vecs, i) for i in k_range]\n",
    "    \n",
    "    # Optionally display the graph of silhouette score by k-value\n",
    "    if show_chart:\n",
    "        fig = plt.plot(k_range, score)\n",
    "        \n",
    "    # Optionally save the graph of silhouette score by k-value to disk\n",
    "    if save_chart:\n",
    "        plt.savefig(chart_file, dpi=300)\n",
    "    \n",
    "    # optimal_k is k value with the highest silhouette score\n",
    "    optimal_k = k_range[np.argmax(score)]\n",
    "    return optimal_k\n",
    "\n",
    "def get_cluster_assignments_kmeans(phrase_vecs:list, k:int):\n",
    "    \"\"\"\n",
    "    K-means clustering.\n",
    "    \n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    # Assign clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(phrase_vecs)\n",
    "    cluster_assignments = kmeans.predict(phrase_vecs)\n",
    "    return cluster_assignments\n",
    "\n",
    "def get_clusters_kmeans(data:DataFrame, k:int = None, show_chart:bool = False):\n",
    "    \"\"\"\n",
    "    Use K-means algorithm to cluster phrase vectors\n",
    "\n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use optimal k if no k-value is specified\n",
    "    if k is None:\n",
    "        k = get_optimal_k(data, show_chart)\n",
    "    \n",
    "    return get_cluster_assignments_kmeans(list(data.vec), k), k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_topics_from_docs(docs:list, topic_count:int):\n",
    "    \"\"\"\n",
    "    Gets a list of `topic_count` topics for each list of tokens in `docs`.\n",
    "    \n",
    "    Returns list\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(text) for text in docs]\n",
    "\n",
    "    # Use Latent Dirichlet Allocation (LDA) to find the main topics \n",
    "    lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "    topics_matrix = lda.show_topics(formatted=False, num_words=topic_count)\n",
    "    topics = list(np.array(topics_matrix[0][1])[:,0])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "def df_to_disk(df:DataFrame, file_name:str, mode:str=\"w\", header:bool=True, sep='\\t'):\n",
    "    \"\"\"\n",
    "    Writes a dataframe to disk as a tab delimited file.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_name), exist_ok=True)\n",
    "    \n",
    "    df.to_csv(file_name, sep=sep, mode=mode, header=header, encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar=\"\",  escapechar=\"\\\\\")\n",
    "    if mode == \"w\":\n",
    "        print(f\"Results saved to {file_name}\")\n",
    "    \n",
    "def sentences_to_disk(data:DataFrame, file_name:str = 'output/DocumentSentenceList.txt'):\n",
    "    \"\"\"\n",
    "    Writes the raw sentences to a file organized by document and sentence number.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    df = data[[\"id\", \"text\"]].copy()\n",
    "    df_to_disk(df, file_name)\n",
    "    \n",
    "    \n",
    "def write_cluster(cluster_rows:DataFrame, file_name:str, mode:str = 'a', header:bool=False):\n",
    "    \"\"\"\n",
    "    Appends the rows for a single cluster to disk.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"    \n",
    "    df_to_disk(cluster_rows, file_name, mode=mode, header=header)\n",
    "    \n",
    "    \n",
    "def clusters_to_disk(data:DataFrame, doc_df:DataFrame, cluster_df:DataFrame, \n",
    "                     file_name:str = 'output/TopicClusterResults.txt'):\n",
    "    \"\"\"\n",
    "    Writes the sentences and phrases to a file organized by cluster and document.\n",
    "\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    # Create a dataframe containing the data to be saved to disk\n",
    "    df = data[[\"cluster\", \"doc_id\", \"sent_id\", \"text\", \"phrase\"]].copy()\n",
    "    doc_names = [doc_df.loc[c].doc_name for c in data.doc_id]\n",
    "    df.insert(loc=3, column='doc_name', value=doc_names)\n",
    "    df.sort_values(by=[\"cluster\", \"doc_id\", \"sent_id\"], inplace=True)\n",
    "\n",
    "    # Write document header\n",
    "    cluster_rows = pd.DataFrame(None, columns=df.columns)\n",
    "    write_cluster(cluster_rows, file_name, mode = 'w', header=True)\n",
    "\n",
    "    # Write each cluster\n",
    "    for c in set(data.cluster):\n",
    "        # Write a cluster header containing the main topics for each cluster\n",
    "        with open(file_name, encoding=\"utf-8\", mode = 'a') as file:\n",
    "            keywords = ', '.join(cluster_df.loc[c, 'topics'])\n",
    "            file.write(f\"Cluster: {c}; Keywords: [{keywords}]\\n\")\n",
    "\n",
    "        # Write the sentences in each cluster\n",
    "        cluster_rows = df[df.cluster == c].copy()\n",
    "        write_cluster(cluster_rows, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n",
      "Converted Untitled.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
