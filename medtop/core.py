# AUTOGENERATED! DO NOT EDIT! File to edit: Core.ipynb (unless otherwise specified).

__all__ = ['import_docs', 'sentences_to_disk', 'create_tfidf', 'append_phrase_column', 'append_vec_column',
           'assign_clusters', 'visualize_umap', 'visualize_mds', 'visualize_svd', 'get_cluster_topics',
           'write_output_to_disk', 'evaluate']

# Cell
import csv
from gensim import corpora, models, matutils
import matplotlib.pyplot as plt
from .internal import *
from .nlp_helpers import *
from .preprocessing import *
import numpy as np
import pandas as pd
import plotly
import plotly.express as px
from sklearn.manifold import MDS
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
import umap.umap_ as umap

# Cell
def import_docs(path_to_file_list):
    "Imports and processes the list of documents contained in the given file"

    # Extract list of files from text document
    with open(path_to_file_list) as file:
        file_list = file.read().strip().split('\n')

    # Create a dataframe containing the doc_id, file name, and raw text of each document
    doc_cols = ["doc_id", "file", "text", "tokens"]
    doc_df = pd.DataFrame(columns=doc_cols)

    # Create a dataframe containing the doc_sent_id, doc_id, sent_id, raw text, tokens, and PoS tags for each sentence
    sent_cols = ["doc_sent_id", "doc_id", "sent_id", "text", "tokens", "pos_tags"]
    data = pd.DataFrame(columns=sent_cols)

    for doc_id, file in enumerate(file_list):
        with open(file, "r") as file_content:
            doc_text = file_content.read()

        sent_tokens, sent_pos, raw_sent = tokenize_and_stem(doc_text)
        for sent_id, _ in enumerate(sent_tokens):
            # Populate a row in data for each sentence in the document
            doc_sent_id = f"doc.{doc_id}.sent.{sent_id}"
            sent_text = raw_sent[sent_id]
            tokens = sent_tokens[sent_id]
            pos_tags = sent_pos[sent_id]
            sent_row = pd.Series([doc_sent_id, doc_id, sent_id, sent_text, tokens, pos_tags], index=sent_cols)
            data = data.append(sent_row, ignore_index=True)

        # Populate a row in doc_df for each file loaded
        doc_tokens = [token for sent in data[data.doc_id==doc_id].tokens for token in sent]
        doc_row = pd.Series([doc_id, file, doc_text, doc_tokens], index=doc_cols)
        doc_df = doc_df.append(doc_row, ignore_index=True)

    return data, doc_df

# Cell
def sentences_to_disk(data, file_name = 'output/DocumentSentenceList.txt'):
    "Writes the raw sentences to a file organized by document and sentence number"
    data.to_csv(file_name, columns=["doc_sent_id", "text"], sep='\t', encoding='utf-8', index=False, quoting=csv.QUOTE_NONE, quotechar="",  escapechar="\\")

# Cell
def create_tfidf(path_to_seed_topics_file_list, input_doc_df = None):
    "Creates a TF-IDF matrix from the tokens in the seed topics documents and optionally, the input corpus"

    # Import seed topics documents
    _, seed_doc_df = import_docs(path_to_seed_topics_file_list)

    # Bag of Words (BoW) is a list of all tokens by document
    bow_docs = list(seed_doc_df.tokens)

    if input_doc_df is not None:
        bow_docs = bow_docs + list(input_doc_df.tokens)

    # Create a TF-IDF matrix using document tokens and gensim
    dictionary = corpora.Dictionary(bow_docs)
    corpus = [dictionary.doc2bow(text) for text in bow_docs]
    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    num_terms = len(corpus_tfidf.obj.idfs)
    tfidf_dense = matutils.corpus2dense(corpus_tfidf, num_terms)
    print(f"Size of TF-IDF: {len(tfidf_dense[0])} Docs, {len(tfidf_dense)} Tokens")

    return tfidf_dense, dictionary

# Cell
def append_phrase_column(data, feature_names, tdm, window_size = 6, include_input_in_tfidf = False):
    """
    Extracts the most expressive phrase of length `window_size` from each sentence and appends them to the input dataframe in a new 'phrase' column.
    Sentences without phrases of sufficient length are set to None.
    """
    top_phrases = []

    if not include_input_in_tfidf:
        token_averages = get_term_max(tdm)

    # Append a phrase column to the 'data' dataframe
    data['phrase'] = None

    # Set 'phrase' on each row of the dataframe
    lambda_func = lambda sent: set_phrase_column(sent, window_size, feature_names, include_input_in_tfidf, tdm, token_averages)
    data.apply(lambda_func, axis=1)
    return data

# Cell
def append_vec_column(method, data, dictionary = None, tfidf = None, dimensions = 2, umap_neighbors = 15, path_to_w2v_bin_file = None, doc_df = None):
    "Creates a word vector for each phrase in the dataframe that isn't None."
    # Append vec column initialized to None
    data['vec'] = None

    if method == "tfidf":
        # Create word vectors with TF-IDF
        assert dictionary is not None, "Optional parameter: 'dictionary' is required for method: 'tfidf'."
        assert tfidf is not None, "Optional parameter: 'tfidf' is required for method: 'tfidf'."
        lambda_func = lambda sent: set_vec_column_tfidf(sent, dictionary, tfidf)
    elif method == "svd":
        # Create word vectors with SVD transformed TF-IDF
        assert dictionary is not None, "Optional parameter: 'dictionary' is required for method: 'svd'."
        assert tfidf is not None, "Optional parameter: 'tfidf' is required for method: 'svd'."
        svd =  TruncatedSVD(n_components = dimensions, random_state = 42)
        tfidf_transf = svd.fit_transform(tfidf)
        lambda_func = lambda sent: set_vec_column_tfidf(sent, dictionary, tfidf_transf)
    elif method == "umap":
        # Create word vectors with UMAP transformed TF-IDF
        assert dictionary is not None, "Optional parameter: 'dictionary' is required for method: 'umap'."
        assert tfidf is not None, "Optional parameter: 'tfidf' is required for method: 'umap'."
        reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42, n_components=dimensions)
        embed = reducer.fit_transform(tfidf)
        lambda_func = lambda sent: set_vec_column_tfidf(sent, dictionary, embed)
    elif method == "pretrained":
        # Create word vectors using a pre-trained Word2Vec model
        assert path_to_w2v_bin_file is not None, "Optional parameter: 'path_to_w2v_bin_file' is required for method: 'pretrained'."
        model = w2v_pretrained(path_to_w2v_bin_file)
        lambda_func = lambda sent: set_vec_column_w2v(sent, model)
    elif method == "local":
        # Generate a Word2Vec model from the input corpus and use it to create word vectors for each phrase
        assert doc_df is not None, "Optional parameter: 'doc_df' is required for method: 'local'."
        large_sent_vec = list(doc_df.tokens)
        model = models.Word2Vec(sg=1, window = 6, max_vocab_size = None, min_count=1, size=10, iter=500)
        model.build_vocab(large_sent_vec)
        model.train(large_sent_vec, total_examples=model.corpus_count, epochs=model.epochs)
        lambda_func = lambda sent: set_vec_column_w2v(sent, model)
    else:
        raise Exception(f"Unrecognized method: '{method}'")

    # Set the vec to a word vector for each sentence with a non-None phrase
    data.apply(lambda_func, axis=1)
    return data

# Cell
def assign_clusters(data, method = "kmeans", dist_metric = "euclidean", k = None, h = None):
    "Clusters the sentences using phrase vectors."
    if method == "kmeans":
        # Cluster using K-means algorithm
        cluster_assignments = get_cluster_assignments_kmeans(data, k)
    elif method == "hac":
        # Cluster using Hierarchical Agglomerative Clustering (HAC)
        cluster_assignments = get_cluster_assignments_hac(data, dist_metric = dist_metric)
    else:
        raise Exception(f"Unrecognized method: '{method}'")

    data['cluster'] = cluster_assignments
    return data

# Cell
def visualize_umap(just_phrase_ids, cluster_assignments, just_phrase_text, dist, umap_neighbors = 15, save_chart = False, chart_file = "UMAP.html"):
    "Visualize the clusters using UMAP"
    reducer = umap.UMAP(n_neighbors=umap_neighbors, min_dist=.1, metric='cosine', random_state=42)
    embedding = reducer.fit_transform(dist)
    df = pd.DataFrame(dict(label=just_phrase_ids, UMAP_cluster=cluster_assignments, phrase=just_phrase_text, x=embedding[:, 0], y=embedding[:, 1]))
    fig = px.scatter(df, x="x", y="y", hover_name="label", color="UMAP_cluster", hover_data=["phrase", "label","UMAP_cluster"], color_continuous_scale='rainbow')
    fig.show()
    if save_chart:
        plotly.offline.plot(fig, filename=chart_file)

def visualize_mds(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = "MDS.html"):
    "Visualize the clusters using Multi-Dimensional Scaling (MDS)"
    mds = MDS(n_components=2, dissimilarity="precomputed", random_state=42)
    pos = mds.fit_transform(dist)
    xs, ys = pos[:, 0], pos[:, 1]
    df = pd.DataFrame(dict(x=xs, y=ys, MDS_cluster=cluster_assignments, label=just_phrase_ids, phrase=just_phrase_text))
    fig = px.scatter(df, x="x", y="y", hover_name="label", color="MDS_cluster", hover_data=["phrase", "label","MDS_cluster"], color_continuous_scale='rainbow')
    fig.show()
    if save_chart:
        plotly.offline.plot(fig, filename=chart_file)

def visualize_svd(just_phrase_ids, cluster_assignments, just_phrase_text, dist, save_chart = False, chart_file = "SVD.html"):
    "Visualize the clusters using Singular Value Decomposition (SVD)"
    svd2d =  TruncatedSVD(n_components = 2, random_state = 42).fit_transform(dist)
    df = pd.DataFrame(dict(label=just_phrase_ids, SVD_cluster=cluster_assignments, phrase=just_phrase_text, x=svd2d[:, 0], y=svd2d[:, 1]))
    fig = px.scatter(df, x="x", y="y", hover_name="label", color="SVD_cluster", hover_data=["phrase", "label","SVD_cluster"], color_continuous_scale='rainbow')
    fig.show()
    if save_chart:
        plotly.offline.plot(fig, filename=chart_file)

# Cell
def get_cluster_topics(cluster_assignments, just_phrase_ids, my_docs):
    "Gets a list of the main topics for each cluster"
    cluster_topics = []
    for c in set(cluster_assignments):
        sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)

        ## sent_coords has each element formatted as [doc number, sentence number]
        cluster_terms = []
        for doc_id, sent_id in sent_coords:
            sent = my_docs[doc_id][sent_id]
            # This isn't used. Can we remove?
#             pos = my_docs_pos[doc_id][sent_id]
            cluster_terms.append(sent)

        cluster_topics.append(cluster_terms)

    main_cluster_topics = []
    for doc_topics in cluster_topics:
        #Latent Dirichlet Allocation implementation with Gensim
        dictionary = corpora.Dictionary(doc_topics)
        corpus = [dictionary.doc2bow(text) for text in doc_topics]
        lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary)
        topics_matrix = lda.show_topics(formatted=False, num_words=10)

        topics_ary = list(np.array(topics_matrix[0][1])[:,0])
        main_cluster_topics.append(topics_ary)
    return main_cluster_topics

# Cell
def write_output_to_disk(main_cluster_topics, just_phrase_ids, cluster_assignments, raw_sentences,
                         doc_top_phrases, file_list, file_name = "TopicClusterResults.txt"):
    "Creates a CSV file listing cluster_id/doc_id/doc_name/sentence_id/sentence_text/sentences_phrases"
    with open(file_name, "w") as outfile:
        for c, topics in enumerate(main_cluster_topics):
            outfile.write("Cluster\tDocument.Num\tDocument.Name\tSentence.Num\tSentence.Text\nPhrase.Text")
            sent_coords = get_cluster_sent_coords(c, just_phrase_ids, cluster_assignments)
            outfile.write(f"Cluster {c} Keywords: {', '.join(topics)}\n")

            ## sent_coords has each element formatted as [doc number, sentence number]
            for doc_id, sent_id in sent_coords:
                if doc_id >= 0 and sent_id >= 0:
                    outfile.write(f"{c}\t{doc_id}\t{file_list[doc_id]}\t{sent_id}\t{raw_sentences[doc_id][sent_id]}\t{doc_top_phrases[doc_id][sent_id]}\n")
                else:
                    #QUESTION: What are we trying to do here? When would this be hit?
                    outfile.write(f"{c}\t{doc_id, sent_id}\t{doc_id, sent_id}\t{doc_id, sent_id}\t{doc_id, sent_id}\t{doc_id, sent_id}\n")

# Cell
def evaluate(just_phrase_ids, cluster_assignments, just_phrase_text,
             gold_file = "data/GOLD_Expert_2019.03.12_TestCorpus_AMY.txt", output_file = "output/EvaluationResults.txt"):
    "Evaluate precision, recall, and F1 against a gold standard dataset. Write results to a file"

    df = pd.DataFrame(dict(label=just_phrase_ids, cluster=cluster_assignments, phrase=just_phrase_text))

    # Process the gold standard file into a list of IDs and classification groups
    with open(gold_file) as file:
        gold_list = file.read().strip().split('\n')
    gold_array = np.array([x.split('\t') for x in gold_list])
    ids, group = gold_array.T
    baseline = list(np.unique(group)) # This is the list of classes ['Confidence', 'Overwhelmed', 'Supportive Environment', 'System Issues']

    # Dictionary of IDs corresponding to each class
    gold_clusters = {}
    for clazz in baseline:
        # List of IDs (doc.#.sent.#) corresponding to a given class
        gold_clusters[clazz] = gold_array[gold_array[:,1] == clazz][:,0]

    #TODO: This can be combined with the for loop above
    closest_to_gold = {}
    for b in baseline:
        max_overlap = 0
        max_overlap_cluster = -1
        cluster_ids = list(set(cluster_assignments))
        just_phrase_ids

        for c in cluster_ids:
            overlap = len(set(gold_clusters[b]).intersection(set(df.label[df.cluster==c])))
            if overlap > max_overlap:
                max_overlap = overlap
                max_overlap_cluster = c
        closest_to_gold[b] = max_overlap_cluster

    #TODO: I think this could also be combined with the above FOR loop
    # Write results to a file
    with open(output_file, "w") as eout:
        eout.write("Gold Concept\tGold Concept Members\tClosest Cluster Num\tClosest Cluster Members\tTP\tFP\tFN\tP\tR\tF1\n")
        for g in gold_clusters.keys():
            ## We only want to do these calculations on sentences that are in the gold file.
            ## Get closest cluster list of sentences that are ONLY in gold
            closest = set(df.label[df.cluster==closest_to_gold[g]]).intersection(set(ids))
            gold = set(gold_clusters[g])
            TP = len(gold.intersection(closest))
            FP = len(closest - gold)
            FN = len(gold - closest)
            P = round(TP/(TP+FP), 3) if (TP+FP) > 0 else float("Nan")
            R = round(TP/(TP+FN), 3) if (TP+FN) > 0 else float("Nan")
            F1 = round(2*((P*R)/(P+R)), 3) if (P+R) > 0 else float("Nan")
            eout.write(f"{g}\t{gold}\t{closest_to_gold[g]}\t{closest}\t{TP}\t{FP}\t{FN}\t{P}\t{R}\t{F1}\n")