# AUTOGENERATED! DO NOT EDIT! File to edit: preprocessing.ipynb (unless otherwise specified).

__all__ = ['decontracted', 'tokenize_and_stem2', 'tokenize_and_stem']

# Cell
import nltk
from nltk.tokenize import WhitespaceTokenizer
from nltk.corpus import stopwords
import string
import re

# Cell
def decontracted(phrase):
    """
    Removes contractions from `phrase`\n
    Obtainined from https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python
    """
    # specific
    phrase = re.sub(r"won\'t", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)
    phrase = re.sub(r"hadn\'t", "had not", phrase)
    phrase = re.sub(r"doesnt", "does not", phrase)
    phrase = re.sub(r"youre", "you are", phrase)
    phrase = re.sub(r"dont", "do not", phrase)
    phrase = re.sub(r"im\s", "i am", phrase)
    phrase = re.sub(r"ive\s", "i have", phrase)

    phrase = re.sub(r"won\’t", "will not", phrase)
    phrase = re.sub(r"can\’t", "can not", phrase)
    phrase = re.sub(r"hadn\’t", "had not", phrase)
    phrase = re.sub(r"dont\s", "do not", phrase)


    # general
    #phrase = re.sub(r"n\'t", " not", phrase)
    #phrase = re.sub(r"\'re", " are", phrase)
    #phrase = re.sub(r"\'s", " is", phrase)
    #phrase = re.sub(r"\'d", " would", phrase)
    #phrase = re.sub(r"\'ll", " will", phrase)
    #phrase = re.sub(r"\'t", " not", phrase)
    #phrase = re.sub(r"\'ve", " have", phrase)
    #phrase = re.sub(r"\'m", " am", phrase)

    phrase = re.sub(r"n\’t", " not", phrase)
    phrase = re.sub(r"\’re", " are", phrase)
    phrase = re.sub(r"\’s", " is", phrase)
    phrase = re.sub(r"\’d", " would", phrase)
    phrase = re.sub(r"\’ll", " will", phrase)
    phrase = re.sub(r"\’t", " not", phrase)
    phrase = re.sub(r"\’ve", " have", phrase)
    phrase = re.sub(r"\’m", " am", phrase)

    phrase = re.sub(r"/", " ", phrase) ### I added this line
    return phrase

# Cell
def tokenize_and_stem2(text):
    split_sent, split_sent_pos, split_sent_loc, my_tokens, my_pos_tags, my_loc, full_sent, full_pos, full_loc, raw_sent = tokenize_and_stem(text)
    return my_tokens

# Cell
def tokenize_and_stem(text):
    "Parse out sentences, remove contractions, tokenize by white space, and remove all punctuation, and lemmatize tokens"
    lemmatizer = nltk.WordNetLemmatizer()
    my_tokens = []
    my_pos_tags = []
    my_loc = []
    full_sent = []
    full_pos = []
    full_loc = []
    raw_sent = []
    stop_words = set(stopwords.words('english')) | {"patient","mrs","hi","ob","1am","4month","o2","ed",
                                                    "ecmo","m3","ha","3rd","ai","csicu","wa","first",
                                                    "second","third","fourth","etc","eg","thus",
                                                    ",",".","'","(",")","!","...","'m","'s",'"',"?", "`",
                                                    "say","many","things","new","much","get","really","since",
                                                    "way","also","one","two","three","four","five","six","week","day",
                                                    "month","year","would","could","should","like","im","thing","v","u","d","g"}
    table  = str.maketrans(' ', ' ', string.punctuation+"“"+"”")
    sent = nltk.sent_tokenize(text)
    split_sent = []
    split_sent_pos = []
    split_sent_loc = []

    #For each sentence in document get back the list of tokenized words with contractions normalized and punctuation removed
    for s in sent:
        raw_sent.append(s)
        tokenized = WhitespaceTokenizer().tokenize(decontracted(s).translate(table))
        tags = nltk.pos_tag(tokenized)
        lemma = [lemmatizer.lemmatize(t) for t in tokenized]
        #convert all remaining tokens to lowercase
        f2 = [w.lower() for w in lemma]

        loc2 = list(range(0, len(f2)))

        #remove stopwords and some punctuation
        f3 = []
        t3 = []
        loc3 = []
        for w in range(0,len(f2)):
            if f2[w] not in stop_words:
                f3.append(f2[w])
                t3.append(tags[w])
                loc3.append(loc2[w])

        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
        filtered_tokens = []
        filtered_tags = []
        filtered_loc = []
        for t in range(0, len(f3)):
            if re.search('[a-zA-Z]', f3[t]):
                filtered_tokens.append(f3[t])
                filtered_tags.append(t3[t])
                filtered_loc.append(loc3[t])
        #print(filtered_loc)

        split_sent.append(filtered_tokens)
        split_sent_pos.append(filtered_tags)
        split_sent_loc.append(filtered_loc)
        my_tokens = my_tokens + filtered_tokens
        my_pos_tags = my_pos_tags + filtered_tags
        my_loc = my_loc + filtered_loc

        full_sent.append(f2)
        full_pos.append(tags)
        full_loc.append(loc2)

        #print(split_sent_loc)
    return split_sent, split_sent_pos, split_sent_loc, my_tokens, my_pos_tags, my_loc, full_sent, full_pos, full_loc, raw_sent