{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import spacy\n",
    "from pandas import DataFrame\n",
    "import topex.internal as internal\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\",\"ner\"])\n",
    "nlp.add_pipe(nlp.create_pipe(\"sentencizer\")) # Add sentence break parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Methods for preprocessing raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def token_filter(token, stopwords:list, custom_stopwords_only:bool=False):\n",
    "    \"Filters out stopwords and tokens without alpha characters\"\n",
    "    include_token = any(map(token.shape_.__contains__, ['X','x'])) and token.lemma_ not in stopwords and token.text not in stopwords\n",
    "    \n",
    "    if not custom_stopwords_only and include_token:\n",
    "        include_token = not token.is_stop\n",
    "    \n",
    "    return include_token\n",
    "\n",
    "def preprocess_docs(doc_df:DataFrame, save_results:bool=False, file_name:str=None, stop_words_file:str=None,\n",
    "                stop_words_list:list=None, custom_stopwords_only:bool=False):\n",
    "    \"\"\"\n",
    "    Imports and pre-processes the documents from the `raw_docs` dataframe\n",
    "    Document pre-processing is handled in [`tokenize_and_stem`](/topex/preprocessing#tokenize_and_stem).\n",
    "    `path_to_file_list` is a path to a text file containing a list of files to be processed separated by line breaks.\n",
    "    Returns (DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    # 1) Get Stopwords\n",
    "    stopwords = get_stop_words(stop_words_file=stop_words_file,stop_words_list=stop_words_list)\n",
    "    nlp.Defaults.stop_words |= set(stopwords)\n",
    "\n",
    "    # 2) Process docs with spaCy\n",
    "    texts = list(doc_df.text)\n",
    "    docs = list(nlp.pipe(texts))\n",
    "\n",
    "    # 3) Create DataFrame of documents\n",
    "    # Remove stopwords and any token without alpha characters\n",
    "    doc_df['tokens'] = [[token.lemma_.lower() for token in doc if token_filter(token,stopwords,custom_stopwords_only)] for doc in docs]\n",
    "\n",
    "    # 4) Create DataFrame of sentences\n",
    "    rows = []\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        for sent_id, sent in enumerate(doc.sents):\n",
    "            uid = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            # Remove stopwords and any token without alpha characters\n",
    "            sent_tokens = list([token for token in sent if token_filter(token,stopwords,custom_stopwords_only)])\n",
    "            text = sent.text\n",
    "            lemmas = [t.lemma_.lower() for t in sent_tokens]\n",
    "            tags = [t.pos_ for t in sent_tokens]\n",
    "            rows.append((uid,doc_id,sent_id,text,lemmas,tags))\n",
    "    data = DataFrame(rows, columns=['id','doc_id','sent_id','text','tokens','pos_tags'])\n",
    "\n",
    "    # Optionally save the results to disk\n",
    "    if save_results:\n",
    "        internal.sentences_to_disk(data, file_name)\n",
    "\n",
    "    return data, doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_stop_words(stop_words_file:str=None, stop_words_list:list=None):\n",
    "    \"Gets a list of all custom stop words\"\n",
    "   # Load custom stop words from file if given\n",
    "    custom_stop_words = set()\n",
    "    if stop_words_file is not None:\n",
    "        if os.path.isfile(stop_words_file):\n",
    "            with open(stop_words_file, encoding=\"utf-8\") as file:\n",
    "                custom_stop_words |= set(file.read().strip().split('\\n'))\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        custom_stop_words |= set(stop_words_list)\n",
    "\n",
    "    return list(set(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
