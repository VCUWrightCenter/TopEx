{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import spacy\n",
    "from pandas import DataFrame\n",
    "import topex.internal as internal\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_sci_sm', disable=[\"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"mesh\"})\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> Methods for preprocessing raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def token_filter(token, stopwords:list, custom_stopwords_only:bool=False):\n",
    "    \"\"\"\n",
    "    Checks if the given token both has alpha characters and is not a stopword\n",
    "    Returns bool\n",
    "    \"\"\"\n",
    "    include_token = any(map(token.shape_.__contains__, ['X','x'])) and token.lemma_ not in stopwords \\\n",
    "        and token.text.lower() not in stopwords\n",
    "\n",
    "    if not custom_stopwords_only and include_token:\n",
    "        include_token = not token.is_stop\n",
    "\n",
    "    return include_token\n",
    "\n",
    "def normalize_entity(entity):\n",
    "    \"\"\"\n",
    "    For a given entity extracted via NER, attempts to normalize to a canonical name\n",
    "    if available, otherwise returns the lemmatized entity.\n",
    "    Returns string\n",
    "    \"\"\"\n",
    "    name = entity.lemma_\n",
    "    if len(entity._.kb_ents) > 0 and entity._.kb_ents[0][1] > .8:\n",
    "       cui = entity._.kb_ents[0][0]\n",
    "       linked = linker.kb.cui_to_entity[cui]\n",
    "       name = linked.canonical_name\n",
    "    return name.lower()\n",
    "\n",
    "def preprocess_docs(doc_df:DataFrame, save_results:bool=False, file_name:str=None, stop_words_file:str=None,\n",
    "                stop_words_list:list=None, custom_stopwords_only:bool=False, ner:bool=False):\n",
    "    \"\"\"\n",
    "    Imports and pre-processes the documents from the `raw_docs` dataframe\n",
    "    Document pre-processing is handled in [`tokenize_and_stem`](/topex/preprocessing#tokenize_and_stem).\n",
    "    `path_to_file_list` is a path to a text file containing a list of files to be processed separated by line breaks.\n",
    "    `ner` runs a biomedical NER pipeline over the input and clusters on extracted entities rather than tokens.\n",
    "    Returns (DataFrame, DataFrame)\n",
    "    \"\"\"\n",
    "    # 1) Get Stopwords\n",
    "    stopwords = get_stop_words(stop_words_file=stop_words_file,stop_words_list=stop_words_list)\n",
    "\n",
    "    # 2) Process docs with spaCy\n",
    "    texts = list(doc_df.text)\n",
    "    docs = list(nlp.pipe(texts))\n",
    "\n",
    "    # 3) Create DataFrame of documents\n",
    "    # Remove stopwords and any token without alpha characters\n",
    "    doc_df['tokens'] = [[token.lemma_.lower() for token in doc if token_filter(token,stopwords,custom_stopwords_only)] for doc in docs]\n",
    "\n",
    "    # 4) Create DataFrame of sentences\n",
    "    rows = []\n",
    "    corpus_tokens = []\n",
    "    for doc_id, doc in tqdm(enumerate(docs), total=len(docs)):\n",
    "        doc_tokens = []\n",
    "        for sent_id, sent in enumerate(doc.sents):\n",
    "            uid = f\"doc.{doc_id}.sent.{sent_id}\"\n",
    "            if ner==True:\n",
    "                # Perform NER instead of tokenization\n",
    "                sent_tokens = [normalize_entity(e) for e in sent.ents]\n",
    "                lemmas = sent_tokens\n",
    "                tags = ['NOUN' for t in sent_tokens]\n",
    "            else:\n",
    "                # Remove stopwords and any token without alpha characters\n",
    "                sent_tokens = list([token for token in sent if token_filter(token,stopwords,custom_stopwords_only=False)])\n",
    "                lemmas = [t.lemma_.lower() for t in sent_tokens]\n",
    "                tags = [t.pos_ for t in sent_tokens]\n",
    "            rows.append((uid,doc_id,sent_id,sent.text,lemmas,tags))\n",
    "            doc_tokens = doc_tokens + sent_tokens\n",
    "        corpus_tokens.append(doc_tokens)\n",
    "    data = DataFrame(rows, columns=['id','doc_id','sent_id','text','tokens','pos_tags'])\n",
    "\n",
    "    # Optionally save the results to disk\n",
    "    if save_results:\n",
    "        internal.sentences_to_disk(data, file_name)\n",
    "\n",
    "    return data, doc_df\n",
    "\n",
    "def get_stop_words(stop_words_file:str=None, stop_words_list:list=None):\n",
    "    \"\"\"\n",
    "    Gets a list of all stop words.\n",
    "    Returns list(string)\n",
    "    \"\"\"\n",
    "   # Load custom stop words from file if given\n",
    "    custom_stop_words = set()\n",
    "    if stop_words_file is not None:\n",
    "        if os.path.isfile(stop_words_file):\n",
    "            with open(stop_words_file, encoding=\"utf-8\") as file:\n",
    "                custom_stop_words |= set(file.read().strip().split('\\n'))\n",
    "\n",
    "    if stop_words_list is not None:\n",
    "        custom_stop_words |= set(stop_words_list)\n",
    "\n",
    "    return list(set([w.lower() for w in custom_stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted internal.ipynb.\n",
      "Converted preprocessing.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TopExEnv]",
   "language": "python",
   "name": "conda-env-TopExEnv-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
